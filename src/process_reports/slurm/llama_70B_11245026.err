ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100, compute capability 9.0, VMM: yes
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100, compute capability 9.0, VMM: yes
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100, compute capability 9.0, VMM: yes
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA H100, compute capability 9.0, VMM: yes
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA H100) - 94819 MiB free
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA H100) - 94819 MiB free
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA H100) - 94819 MiB free
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA H100) - 94819 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 724 tensors from ../../../hf_models/Llama-3.3-70B-Instruct-GGUF/Llama-3.3-70B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.3 70B Instruct
llama_model_loader: - kv   3:                       general.organization str              = Meta Llama
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.3
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                          llama.block_count u32              = 80
llama_model_loader: - kv   8:                       llama.context_length u32              = 131072
llama_model_loader: - kv   9:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: loaded meta data with 30 key-value pairs and 724 tensors from ../../../hf_models/Llama-3.3-70B-Instruct-GGUF/Llama-3.3-70B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.3 70B Instruct
llama_model_loader: - kv   3:                       general.organization str              = Meta Llama
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.3
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                          llama.block_count u32              = 80
llama_model_loader: - kv   8:                       llama.context_length u32              = 131072
llama_model_loader: - kv   9:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: loaded meta data with 30 key-value pairs and 724 tensors from ../../../hf_models/Llama-3.3-70B-Instruct-GGUF/Llama-3.3-70B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.3 70B Instruct
llama_model_loader: - kv   3:                       general.organization str              = Meta Llama
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.3
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                          llama.block_count u32              = 80
llama_model_loader: - kv   8:                       llama.context_length u32              = 131072
llama_model_loader: - kv   9:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: loaded meta data with 30 key-value pairs and 724 tensors from ../../../hf_models/Llama-3.3-70B-Instruct-GGUF/Llama-3.3-70B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.3 70B Instruct
llama_model_loader: - kv   3:                       general.organization str              = Meta Llama
llama_model_loader: - kv   4:                           general.finetune str              = Instruct
llama_model_loader: - kv   5:                           general.basename str              = Llama-3.3
llama_model_loader: - kv   6:                         general.size_label str              = 70B
llama_model_loader: - kv   7:                          llama.block_count u32              = 80
llama_model_loader: - kv   8:                       llama.context_length u32              = 131072
llama_model_loader: - kv   9:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
init_tokenizer: initializing tokenizer for type 2
init_tokenizer: initializing tokenizer for type 2
init_tokenizer: initializing tokenizer for type 2
init_tokenizer: initializing tokenizer for type 2
load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG
load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG
load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG
load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG
load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG
load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG
load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG
load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG
load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG
load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG
load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG
load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG
load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG
load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG
load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG
load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG
load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG
load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG
load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG
load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG
load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG
load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG
load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG
load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG
load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG
load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG
load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG
load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG
load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG
load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG
load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG
load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG
load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG
load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG
load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG
load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG
load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG
load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG
load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG
load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG
load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG
load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG
load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG
load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG
load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG
load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG
load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG
load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG
load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG
load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG
load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG
load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG
load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG
load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG
load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG
load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG
load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG
load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG
load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG
load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG
load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG
load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG
load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG
load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG
load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG
load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG
load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG
load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG
load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG
load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG
load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG
load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG
load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG
load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG
load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG
load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG
load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG
load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG
load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG
load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG
load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG
load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG
load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG
load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG
load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG
load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG
load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG
load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG
load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG
load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG
load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG
load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG
load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG
load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG
load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG
load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG
load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG
load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG
load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG
load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG
load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG
load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG
load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG
load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG
load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG
load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG
load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG
load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG
load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG
load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG
load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG
load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG
load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG
load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG
load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG
load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG
load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG
load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG
load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG
load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG
load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG
load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG
load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG
load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG
load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG
load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG
load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG
load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG
load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG
load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG
load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG
load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG
load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG
load: control token: 128007 '<|end_header_id|>' is not marked as EOG
load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG
load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG
load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG
load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG
load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG
load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG
load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG
load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG
load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG
load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG
load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG
load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG
load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG
load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG
load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG
load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG
load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG
load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG
load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG
load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG
load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG
load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG
load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG
load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG
load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG
load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG
load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG
load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG
load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG
load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG
load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG
load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG
load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG
load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG
load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG
load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG
load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG
load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG
load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG
load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG
load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG
load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG
load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG
load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG
load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG
load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG
load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG
load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG
load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG
load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG
load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG
load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG
load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG
load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG
load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG
load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG
load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG
load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG
load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG
load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG
load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG
load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG
load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG
load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG
load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG
load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG
load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG
load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG
load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG
load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG
load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG
load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG
load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG
load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG
load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG
load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG
load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG
load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG
load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG
load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG
load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG
load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG
load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG
load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG
load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG
load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG
load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG
load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG
load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG
load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG
load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG
load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG
load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG
load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG
load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG
load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG
load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG
load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG
load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG
load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG
load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG
load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG
load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG
load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG
load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG
load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG
load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG
load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG
load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG
load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG
load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG
load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG
load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG
load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG
load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG
load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG
load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG
load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG
load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG
load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG
load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG
load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG
load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG
load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG
load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG
load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG
load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG
load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG
load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG
load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG
load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG
load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG
load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG
load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG
load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG
load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG
load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG
load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG
load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG
load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG
load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG
load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG
load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG
load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG
load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG
load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG
load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG
load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG
load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG
load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG
load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG
load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG
load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG
load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG
load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG
load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG
load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG
load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG
load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG
load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG
load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG
load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG
load: control token: 128007 '<|end_header_id|>' is not marked as EOG
load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG
load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG
load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG
load: control token: 128001 '<|end_of_text|>' is not marked as EOG
load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG
load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG
load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG
load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG
load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG
load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG
load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG
load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG
load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG
load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG
load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG
load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG
load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG
load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG
load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG
load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG
load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG
load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG
load: control token: 128006 '<|start_header_id|>' is not marked as EOG
load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG
load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG
load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG
load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG
load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG
load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG
load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG
load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG
load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG
load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG
load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG
load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG
load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG
load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG
load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG
load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG
load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG
load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG
load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG
load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG
load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG
load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG
load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG
load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG
load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG
load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG
load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG
load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG
load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG
load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG
load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG
load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG
load: control token: 128001 '<|end_of_text|>' is not marked as EOG
load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG
load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG
load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG
load: control token: 128000 '<|begin_of_text|>' is not marked as EOG
load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG
load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG
load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG
load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG
load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG
load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG
load: control token: 128006 '<|start_header_id|>' is not marked as EOG
load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG
load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG
load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG
load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG
load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG
load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG
load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG
load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG
load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG
load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG
load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG
load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG
load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG
load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG
load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG
load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG
load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG
load: control token: 128010 '<|python_tag|>' is not marked as EOG
load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG
load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG
load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG
load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG
load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG
load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG
load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG
load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG
load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG
load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG
load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG
load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG
load: control token: 128000 '<|begin_of_text|>' is not marked as EOG
load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG
load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG
load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG
load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG
load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG
load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG
load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG
load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG
load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG
load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG
load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG
load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG
load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG
load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG
load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG
load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG
load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG
load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG
load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG
load: control token: 128010 '<|python_tag|>' is not marked as EOG
load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG
load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG
load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG
load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG
load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG
load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG
load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG
load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG
load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG
load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG
load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG
load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG
load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG
load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG
load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG
load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG
load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG
load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG
load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG
load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG
load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG
load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG
load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG
load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG
load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG
load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG
load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG
load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG
load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG
load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG
load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG
load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG
load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG
load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG
load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG
load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG
load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG
load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG
load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG
load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG
load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG
load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG
load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG
load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG
load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG
load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG
load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG
load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG
load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG
load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG
load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG
load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG
load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG
load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG
load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG
load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG
load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG
load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG
load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG
load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG
load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG
load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG
load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG
load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG
load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG
load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG
load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG
load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG
load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG
load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG
load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG
load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG
load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG
load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG
load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG
load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG
load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG
load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG
load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG
load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG
load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG
load: special tokens cache size = 256
load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG
load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG
load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG
load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG
load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG
load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG
load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG
load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG
load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG
load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG
load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG
load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG
load: special tokens cache size = 256
load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG
load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG
load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG
load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG
load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG
load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG
load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG
load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG
load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG
load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG
load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG
load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG
load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG
load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG
load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG
load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG
load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG
load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG
load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG
load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG
load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG
load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG
load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG
load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG
load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG
load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG
load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG
load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG
load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG
load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG
load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG
load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG
load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG
load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG
load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG
load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG
load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG
load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG
load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG
load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG
load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG
load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG
load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG
load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG
load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG
load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG
load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG
load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG
load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG
load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG
load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG
load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG
load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG
load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG
load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG
load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG
load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG
load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG
load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG
load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG
load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG
load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG
load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG
load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG
load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG
load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG
load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG
load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG
load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG
load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG
load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG
load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG
load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG
load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG
load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG
load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG
load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG
load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG
load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG
load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG
load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG
load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG
load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG
load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG
load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG
load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG
load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG
load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG
load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG
load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG
load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG
load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG
load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG
load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG
load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG
load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG
load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG
load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG
load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG
load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG
load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG
load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG
load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG
load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG
load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG
load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG
load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG
load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG
load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG
load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG
load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG
load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG
load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG
load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG
load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG
load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG
load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG
load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG
load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG
load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG
load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG
load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG
load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG
load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG
load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG
load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG
load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG
load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG
load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG
load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG
load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG
load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG
load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG
load: control token: 128007 '<|end_header_id|>' is not marked as EOG
load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG
load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG
load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG
load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG
load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG
load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG
load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG
load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG
load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG
load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG
load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG
load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG
load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG
load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG
load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG
load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG
load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG
load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG
load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG
load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG
load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG
load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG
load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG
load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG
load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG
load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG
load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG
load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG
load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG
load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG
load: control token: 128001 '<|end_of_text|>' is not marked as EOG
load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG
load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG
load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG
load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG
load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG
load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG
load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG
load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG
load: control token: 128006 '<|start_header_id|>' is not marked as EOG
load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG
load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG
load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG
load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG
load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG
load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG
load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG
load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG
load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG
load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG
load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG
load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG
load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG
load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG
load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG
load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG
load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG
load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG
load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG
load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG
load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG
load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG
load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG
load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG
load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG
load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG
load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG
load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG
load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG
load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG
load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG
load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG
load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG
load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG
load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG
load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG
load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG
load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG
load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG
load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG
load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG
load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG
load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG
load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG
load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG
load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG
load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG
load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG
load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG
load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG
load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG
load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG
load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG
load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG
load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG
load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG
load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG
load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG
load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG
load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG
load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG
load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG
load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG
load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG
load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG
load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG
load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG
load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG
load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG
load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG
load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG
load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG
load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG
load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG
load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG
load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG
load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG
load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG
load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG
load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG
load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG
load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG
load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG
load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG
load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG
load: control token: 128000 '<|begin_of_text|>' is not marked as EOG
load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG
load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG
load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG
load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG
load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG
load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG
load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG
load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG
load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG
load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG
load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG
load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG
load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG
load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG
load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG
load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG
load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG
load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG
load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG
load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG
load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG
load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG
load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG
load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG
load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG
load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG
load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG
load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG
load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG
load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG
load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG
load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG
load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG
load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG
load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG
load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG
load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG
load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG
load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG
load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG
load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG
load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG
load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG
load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG
load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG
load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG
load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG
load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG
load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG
load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG
load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG
load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG
load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG
load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG
load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG
load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG
load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG
load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG
load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG
load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG
load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG
load: control token: 128010 '<|python_tag|>' is not marked as EOG
load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG
load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG
load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG
load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG
load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG
load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG
load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG
load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG
load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG
load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG
load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG
load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG
load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG
load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG
load: control token: 128007 '<|end_header_id|>' is not marked as EOG
load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG
load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG
load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG
load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG
load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG
load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG
load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG
load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG
load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG
load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG
load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG
load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG
load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG
load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG
load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG
load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG
load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG
load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG
load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG
load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG
load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG
load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG
load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG
load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG
load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG
load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG
load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG
load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG
load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG
load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG
load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG
load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG
load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG
load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG
load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG
load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG
load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG
load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG
load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG
load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG
load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG
load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG
load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG
load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG
load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG
load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG
load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG
load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG
load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG
load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG
load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG
load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG
load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG
load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG
load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG
load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG
load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG
load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG
load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG
load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG
load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG
load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG
load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG
load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG
load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG
load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG
load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG
load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG
load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG
load: control token: 128001 '<|end_of_text|>' is not marked as EOG
load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG
load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG
load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG
load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG
load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG
load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG
load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG
load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG
load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG
load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG
load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG
load: control token: 128006 '<|start_header_id|>' is not marked as EOG
load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG
load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG
load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG
load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG
load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG
load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG
load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG
load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG
load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG
load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG
load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG
load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG
load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG
load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG
load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG
load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG
load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG
load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG
load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG
load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG
load: special tokens cache size = 256
load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG
load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG
load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG
load: control token: 128000 '<|begin_of_text|>' is not marked as EOG
load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG
load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG
load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG
load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG
load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG
load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG
load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG
load: control token: 128010 '<|python_tag|>' is not marked as EOG
load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG
load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG
load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG
load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG
load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG
load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG
load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG
load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG
load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG
load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG
load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG
load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG
load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG
load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG
load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG
load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG
load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG
load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG
load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG
load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG
load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG
load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG
load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG
load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG
load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG
load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG
load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG
load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG
load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG
load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG
load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG
load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG
load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG
load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG
load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG
load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG
load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG
load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG
load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG
load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG
load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG
load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG
load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG
load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG
load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG
load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG
load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG
load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG
load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG
load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG
load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG
load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG
load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG
load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG
load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG
load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG
load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Llama 3.3 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128004 '<|finetune_right_pad_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CUDA0
load_tensors: layer   1 assigned to device CUDA0
load_tensors: layer   2 assigned to device CUDA0
load_tensors: layer   3 assigned to device CUDA0
load_tensors: layer   4 assigned to device CUDA0
load_tensors: layer   5 assigned to device CUDA0
load_tensors: layer   6 assigned to device CUDA0
load_tensors: layer   7 assigned to device CUDA0
load_tensors: layer   8 assigned to device CUDA0
load_tensors: layer   9 assigned to device CUDA0
load_tensors: layer  10 assigned to device CUDA0
load_tensors: layer  11 assigned to device CUDA0
load_tensors: layer  12 assigned to device CUDA0
load_tensors: layer  13 assigned to device CUDA0
load_tensors: layer  14 assigned to device CUDA0
load_tensors: layer  15 assigned to device CUDA0
load_tensors: layer  16 assigned to device CUDA0
load_tensors: layer  17 assigned to device CUDA0
load_tensors: layer  18 assigned to device CUDA0
load_tensors: layer  19 assigned to device CUDA0
load_tensors: layer  20 assigned to device CUDA0
load_tensors: layer  21 assigned to device CUDA0
load_tensors: layer  22 assigned to device CUDA0
load_tensors: layer  23 assigned to device CUDA0
load_tensors: layer  24 assigned to device CUDA0
load_tensors: layer  25 assigned to device CUDA0
load_tensors: layer  26 assigned to device CUDA0
load_tensors: layer  27 assigned to device CUDA0
load_tensors: layer  28 assigned to device CUDA0
load_tensors: layer  29 assigned to device CUDA0
load_tensors: layer  30 assigned to device CUDA0
load_tensors: layer  31 assigned to device CUDA0
load_tensors: layer  32 assigned to device CUDA0
load_tensors: layer  33 assigned to device CUDA0
load_tensors: layer  34 assigned to device CUDA0
load_tensors: layer  35 assigned to device CUDA0
load_tensors: layer  36 assigned to device CUDA0
load_tensors: layer  37 assigned to device CUDA0
load_tensors: layer  38 assigned to device CUDA0
load_tensors: layer  39 assigned to device CUDA0
load_tensors: layer  40 assigned to device CUDA0
load_tensors: layer  41 assigned to device CUDA0
load_tensors: layer  42 assigned to device CUDA0
load_tensors: layer  43 assigned to device CUDA0
load_tensors: layer  44 assigned to device CUDA0
load_tensors: layer  45 assigned to device CUDA0
load_tensors: layer  46 assigned to device CUDA0
load_tensors: layer  47 assigned to device CUDA0
load_tensors: layer  48 assigned to device CUDA0
load_tensors: layer  49 assigned to device CUDA0
load_tensors: layer  50 assigned to device CUDA0
load_tensors: layer  51 assigned to device CUDA0
load_tensors: layer  52 assigned to device CUDA0
load_tensors: layer  53 assigned to device CUDA0
load_tensors: layer  54 assigned to device CUDA0
load_tensors: layer  55 assigned to device CUDA0
load_tensors: layer  56 assigned to device CUDA0
load_tensors: layer  57 assigned to device CUDA0
load_tensors: layer  58 assigned to device CUDA0
load_tensors: layer  59 assigned to device CUDA0
load_tensors: layer  60 assigned to device CUDA0
load_tensors: layer  61 assigned to device CUDA0
load_tensors: layer  62 assigned to device CUDA0
load_tensors: layer  63 assigned to device CUDA0
load_tensors: layer  64 assigned to device CUDA0
load_tensors: layer  65 assigned to device CUDA0
load_tensors: layer  66 assigned to device CUDA0
load_tensors: layer  67 assigned to device CUDA0
load_tensors: layer  68 assigned to device CUDA0
load_tensors: layer  69 assigned to device CUDA0
load_tensors: layer  70 assigned to device CUDA0
load_tensors: layer  71 assigned to device CUDA0
load_tensors: layer  72 assigned to device CUDA0
load_tensors: layer  73 assigned to device CUDA0
load_tensors: layer  74 assigned to device CUDA0
load_tensors: layer  75 assigned to device CUDA0
load_tensors: layer  76 assigned to device CUDA0
load_tensors: layer  77 assigned to device CUDA0
load_tensors: layer  78 assigned to device CUDA0
load_tensors: layer  79 assigned to device CUDA0
load_tensors: layer  80 assigned to device CUDA0
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Llama 3.3 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128004 '<|finetune_right_pad_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CUDA0
load_tensors: layer   1 assigned to device CUDA0
load_tensors: layer   2 assigned to device CUDA0
load_tensors: layer   3 assigned to device CUDA0
load_tensors: layer   4 assigned to device CUDA0
load_tensors: layer   5 assigned to device CUDA0
load_tensors: layer   6 assigned to device CUDA0
load_tensors: layer   7 assigned to device CUDA0
load_tensors: layer   8 assigned to device CUDA0
load_tensors: layer   9 assigned to device CUDA0
load_tensors: layer  10 assigned to device CUDA0
load_tensors: layer  11 assigned to device CUDA0
load_tensors: layer  12 assigned to device CUDA0
load_tensors: layer  13 assigned to device CUDA0
load_tensors: layer  14 assigned to device CUDA0
load_tensors: layer  15 assigned to device CUDA0
load_tensors: layer  16 assigned to device CUDA0
load_tensors: layer  17 assigned to device CUDA0
load_tensors: layer  18 assigned to device CUDA0
load_tensors: layer  19 assigned to device CUDA0
load_tensors: layer  20 assigned to device CUDA0
load_tensors: layer  21 assigned to device CUDA0
load_tensors: layer  22 assigned to device CUDA0
load_tensors: layer  23 assigned to device CUDA0
load_tensors: layer  24 assigned to device CUDA0
load_tensors: layer  25 assigned to device CUDA0
load_tensors: layer  26 assigned to device CUDA0
load_tensors: layer  27 assigned to device CUDA0
load_tensors: layer  28 assigned to device CUDA0
load_tensors: layer  29 assigned to device CUDA0
load_tensors: layer  30 assigned to device CUDA0
load_tensors: layer  31 assigned to device CUDA0
load_tensors: layer  32 assigned to device CUDA0
load_tensors: layer  33 assigned to device CUDA0
load_tensors: layer  34 assigned to device CUDA0
load_tensors: layer  35 assigned to device CUDA0
load_tensors: layer  36 assigned to device CUDA0
load_tensors: layer  37 assigned to device CUDA0
load_tensors: layer  38 assigned to device CUDA0
load_tensors: layer  39 assigned to device CUDA0
load_tensors: layer  40 assigned to device CUDA0
load_tensors: layer  41 assigned to device CUDA0
load_tensors: layer  42 assigned to device CUDA0
load_tensors: layer  43 assigned to device CUDA0
load_tensors: layer  44 assigned to device CUDA0
load_tensors: layer  45 assigned to device CUDA0
load_tensors: layer  46 assigned to device CUDA0
load_tensors: layer  47 assigned to device CUDA0
load_tensors: layer  48 assigned to device CUDA0
load_tensors: layer  49 assigned to device CUDA0
load_tensors: layer  50 assigned to device CUDA0
load_tensors: layer  51 assigned to device CUDA0
load_tensors: layer  52 assigned to device CUDA0
load_tensors: layer  53 assigned to device CUDA0
load_tensors: layer  54 assigned to device CUDA0
load_tensors: layer  55 assigned to device CUDA0
load_tensors: layer  56 assigned to device CUDA0
load_tensors: layer  57 assigned to device CUDA0
load_tensors: layer  58 assigned to device CUDA0
load_tensors: layer  59 assigned to device CUDA0
load_tensors: layer  60 assigned to device CUDA0
load_tensors: layer  61 assigned to device CUDA0
load_tensors: layer  62 assigned to device CUDA0
load_tensors: layer  63 assigned to device CUDA0
load_tensors: layer  64 assigned to device CUDA0
load_tensors: layer  65 assigned to device CUDA0
load_tensors: layer  66 assigned to device CUDA0
load_tensors: layer  67 assigned to device CUDA0
load_tensors: layer  68 assigned to device CUDA0
load_tensors: layer  69 assigned to device CUDA0
load_tensors: layer  70 assigned to device CUDA0
load_tensors: layer  71 assigned to device CUDA0
load_tensors: layer  72 assigned to device CUDA0
load_tensors: layer  73 assigned to device CUDA0
load_tensors: layer  74 assigned to device CUDA0
load_tensors: layer  75 assigned to device CUDA0
load_tensors: layer  76 assigned to device CUDA0
load_tensors: layer  77 assigned to device CUDA0
load_tensors: layer  78 assigned to device CUDA0
load_tensors: layer  79 assigned to device CUDA0
load_tensors: layer  80 assigned to device CUDA0
load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Llama 3.3 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128004 '<|finetune_right_pad_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CUDA0
load_tensors: layer   1 assigned to device CUDA0
load_tensors: layer   2 assigned to device CUDA0
load_tensors: layer   3 assigned to device CUDA0
load_tensors: layer   4 assigned to device CUDA0
load_tensors: layer   5 assigned to device CUDA0
load_tensors: layer   6 assigned to device CUDA0
load_tensors: layer   7 assigned to device CUDA0
load_tensors: layer   8 assigned to device CUDA0
load_tensors: layer   9 assigned to device CUDA0
load_tensors: layer  10 assigned to device CUDA0
load_tensors: layer  11 assigned to device CUDA0
load_tensors: layer  12 assigned to device CUDA0
load_tensors: layer  13 assigned to device CUDA0
load_tensors: layer  14 assigned to device CUDA0
load_tensors: layer  15 assigned to device CUDA0
load_tensors: layer  16 assigned to device CUDA0
load_tensors: layer  17 assigned to device CUDA0
load_tensors: layer  18 assigned to device CUDA0
load_tensors: layer  19 assigned to device CUDA0
load_tensors: layer  20 assigned to device CUDA0
load_tensors: layer  21 assigned to device CUDA0
load_tensors: layer  22 assigned to device CUDA0
load_tensors: layer  23 assigned to device CUDA0
load_tensors: layer  24 assigned to device CUDA0
load_tensors: layer  25 assigned to device CUDA0
load_tensors: layer  26 assigned to device CUDA0
load_tensors: layer  27 assigned to device CUDA0
load_tensors: layer  28 assigned to device CUDA0
load_tensors: layer  29 assigned to device CUDA0
load_tensors: layer  30 assigned to device CUDA0
load_tensors: layer  31 assigned to device CUDA0
load_tensors: layer  32 assigned to device CUDA0
load_tensors: layer  33 assigned to device CUDA0
load_tensors: layer  34 assigned to device CUDA0
load_tensors: layer  35 assigned to device CUDA0
load_tensors: layer  36 assigned to device CUDA0
load_tensors: layer  37 assigned to device CUDA0
load_tensors: layer  38 assigned to device CUDA0
load_tensors: layer  39 assigned to device CUDA0
load_tensors: layer  40 assigned to device CUDA0
load_tensors: layer  41 assigned to device CUDA0
load_tensors: layer  42 assigned to device CUDA0
load_tensors: layer  43 assigned to device CUDA0
load_tensors: layer  44 assigned to device CUDA0
load_tensors: layer  45 assigned to device CUDA0
load_tensors: layer  46 assigned to device CUDA0
load_tensors: layer  47 assigned to device CUDA0
load_tensors: layer  48 assigned to device CUDA0
load_tensors: layer  49 assigned to device CUDA0
load_tensors: layer  50 assigned to device CUDA0
load_tensors: layer  51 assigned to device CUDA0
load_tensors: layer  52 assigned to device CUDA0
load_tensors: layer  53 assigned to device CUDA0
load_tensors: layer  54 assigned to device CUDA0
load_tensors: layer  55 assigned to device CUDA0
load_tensors: layer  56 assigned to device CUDA0
load_tensors: layer  57 assigned to device CUDA0
load_tensors: layer  58 assigned to device CUDA0
load_tensors: layer  59 assigned to device CUDA0
load_tensors: layer  60 assigned to device CUDA0
load_tensors: layer  61 assigned to device CUDA0
load_tensors: layer  62 assigned to device CUDA0
load_tensors: layer  63 assigned to device CUDA0
load_tensors: layer  64 assigned to device CUDA0
load_tensors: layer  65 assigned to device CUDA0
load_tensors: layer  66 assigned to device CUDA0
load_tensors: layer  67 assigned to device CUDA0
load_tensors: layer  68 assigned to device CUDA0
load_tensors: layer  69 assigned to device CUDA0
load_tensors: layer  70 assigned to device CUDA0
load_tensors: layer  71 assigned to device CUDA0
load_tensors: layer  72 assigned to device CUDA0
load_tensors: layer  73 assigned to device CUDA0
load_tensors: layer  74 assigned to device CUDA0
load_tensors: layer  75 assigned to device CUDA0
load_tensors: layer  76 assigned to device CUDA0
load_tensors: layer  77 assigned to device CUDA0
load_tensors: layer  78 assigned to device CUDA0
load_tensors: layer  79 assigned to device CUDA0
load_tensors: layer  80 assigned to device CUDA0
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Llama 3.3 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128004 '<|finetune_right_pad_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CUDA0
load_tensors: layer   1 assigned to device CUDA0
load_tensors: layer   2 assigned to device CUDA0
load_tensors: layer   3 assigned to device CUDA0
load_tensors: layer   4 assigned to device CUDA0
load_tensors: layer   5 assigned to device CUDA0
load_tensors: layer   6 assigned to device CUDA0
load_tensors: layer   7 assigned to device CUDA0
load_tensors: layer   8 assigned to device CUDA0
load_tensors: layer   9 assigned to device CUDA0
load_tensors: layer  10 assigned to device CUDA0
load_tensors: layer  11 assigned to device CUDA0
load_tensors: layer  12 assigned to device CUDA0
load_tensors: layer  13 assigned to device CUDA0
load_tensors: layer  14 assigned to device CUDA0
load_tensors: layer  15 assigned to device CUDA0
load_tensors: layer  16 assigned to device CUDA0
load_tensors: layer  17 assigned to device CUDA0
load_tensors: layer  18 assigned to device CUDA0
load_tensors: layer  19 assigned to device CUDA0
load_tensors: layer  20 assigned to device CUDA0
load_tensors: layer  21 assigned to device CUDA0
load_tensors: layer  22 assigned to device CUDA0
load_tensors: layer  23 assigned to device CUDA0
load_tensors: layer  24 assigned to device CUDA0
load_tensors: layer  25 assigned to device CUDA0
load_tensors: layer  26 assigned to device CUDA0
load_tensors: layer  27 assigned to device CUDA0
load_tensors: layer  28 assigned to device CUDA0
load_tensors: layer  29 assigned to device CUDA0
load_tensors: layer  30 assigned to device CUDA0
load_tensors: layer  31 assigned to device CUDA0
load_tensors: layer  32 assigned to device CUDA0
load_tensors: layer  33 assigned to device CUDA0
load_tensors: layer  34 assigned to device CUDA0
load_tensors: layer  35 assigned to device CUDA0
load_tensors: layer  36 assigned to device CUDA0
load_tensors: layer  37 assigned to device CUDA0
load_tensors: layer  38 assigned to device CUDA0
load_tensors: layer  39 assigned to device CUDA0
load_tensors: layer  40 assigned to device CUDA0
load_tensors: layer  41 assigned to device CUDA0
load_tensors: layer  42 assigned to device CUDA0
load_tensors: layer  43 assigned to device CUDA0
load_tensors: layer  44 assigned to device CUDA0
load_tensors: layer  45 assigned to device CUDA0
load_tensors: layer  46 assigned to device CUDA0
load_tensors: layer  47 assigned to device CUDA0
load_tensors: layer  48 assigned to device CUDA0
load_tensors: layer  49 assigned to device CUDA0
load_tensors: layer  50 assigned to device CUDA0
load_tensors: layer  51 assigned to device CUDA0
load_tensors: layer  52 assigned to device CUDA0
load_tensors: layer  53 assigned to device CUDA0
load_tensors: layer  54 assigned to device CUDA0
load_tensors: layer  55 assigned to device CUDA0
load_tensors: layer  56 assigned to device CUDA0
load_tensors: layer  57 assigned to device CUDA0
load_tensors: layer  58 assigned to device CUDA0
load_tensors: layer  59 assigned to device CUDA0
load_tensors: layer  60 assigned to device CUDA0
load_tensors: layer  61 assigned to device CUDA0
load_tensors: layer  62 assigned to device CUDA0
load_tensors: layer  63 assigned to device CUDA0
load_tensors: layer  64 assigned to device CUDA0
load_tensors: layer  65 assigned to device CUDA0
load_tensors: layer  66 assigned to device CUDA0
load_tensors: layer  67 assigned to device CUDA0
load_tensors: layer  68 assigned to device CUDA0
load_tensors: layer  69 assigned to device CUDA0
load_tensors: layer  70 assigned to device CUDA0
load_tensors: layer  71 assigned to device CUDA0
load_tensors: layer  72 assigned to device CUDA0
load_tensors: layer  73 assigned to device CUDA0
load_tensors: layer  74 assigned to device CUDA0
load_tensors: layer  75 assigned to device CUDA0
load_tensors: layer  76 assigned to device CUDA0
load_tensors: layer  77 assigned to device CUDA0
load_tensors: layer  78 assigned to device CUDA0
load_tensors: layer  79 assigned to device CUDA0
load_tensors: layer  80 assigned to device CUDA0
load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors: offloading output layer to GPU
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
..load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
.......................................................................................load_tensors: offloading 80 repeating layers to GPU
.load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
............................................................................................................................................................................................................................................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 8192
llama_init_from_model: n_batch       = 512
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 48: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 49: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 50: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 51: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 52: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 53: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 54: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 55: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 56: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 57: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 58: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 59: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 60: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 61: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 62: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 63: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 64: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 65: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 66: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 67: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 68: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 69: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 70: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 71: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 72: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 73: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 74: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 75: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 76: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 77: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 78: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 79: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init:      CUDA0 KV buffer size =  2560.00 MiB
llama_init_from_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     0.49 MiB
.llama_init_from_model:      CUDA0 compute buffer size =   266.50 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    32.01 MiB
llama_init_from_model: graph nodes  = 2247
llama_init_from_model: graph splits = 2
CUDA : ARCHS = 900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 
Model metadata: {'tokenizer.chat_template': '{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = "26 Jul 2024" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- "<|start_header_id|>system<|end_header_id|>\\n\\n" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- "Environment: ipython\\n" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- "Tools: " + builtin_tools | reject(\'equalto\', \'code_interpreter\') | join(", ") + "\\n\\n"}}\n{%- endif %}\n{{- "Cutting Knowledge Date: December 2023\\n" }}\n{{- "Today Date: " + date_string + "\\n\\n" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- "<|eot_id|>" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|start_header_id|>user<|end_header_id|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot_id|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\\n\\n\'+ message[\'content\'] | trim + \'<|eot_id|>\' }}\n    {%- elif \'tool_calls\' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception("This model only supports single tool-calls at once!") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- "<|python_tag|>" + tool_call.name + ".call(" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + \'="\' + arg_val + \'"\' }}\n                {%- if not loop.last %}\n                    {{- ", " }}\n                {%- endif %}\n                {%- endfor %}\n            {{- ")" }}\n        {%- else  %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- \'{"name": "\' + tool_call.name + \'", \' }}\n            {{- \'"parameters": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- "}" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we\'re in ipython mode #}\n            {{- "<|eom_id|>" }}\n        {%- else %}\n            {{- "<|eot_id|>" }}\n        {%- endif %}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' }}\n{%- endif %}\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.value_length': '128', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.padding_token_id': '128004', 'general.basename': 'Llama-3.3', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '64', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Llama 3.3 70B Instruct', 'general.organization': 'Meta Llama', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '70B', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8'}
Available chat formats from metadata: chat_template.default
..
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 8192
llama_init_from_model: n_batch       = 512
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 48: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 49: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 50: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 51: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 52: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 53: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 54: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 55: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 56: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 57: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 58: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 59: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 60: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 61: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 62: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 63: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 64: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 65: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 66: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 67: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 68: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 69: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 70: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 71: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 72: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 73: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 74: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 75: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 76: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 77: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 78: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 79: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init:      CUDA0 KV buffer size =  2560.00 MiB
llama_init_from_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     0.49 MiB
.llama_init_from_model:      CUDA0 compute buffer size =   266.50 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    32.01 MiB
llama_init_from_model: graph nodes  = 2247
llama_init_from_model: graph splits = 2
CUDA : ARCHS = 900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 
Model metadata: {'tokenizer.chat_template': '{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = "26 Jul 2024" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- "<|start_header_id|>system<|end_header_id|>\\n\\n" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- "Environment: ipython\\n" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- "Tools: " + builtin_tools | reject(\'equalto\', \'code_interpreter\') | join(", ") + "\\n\\n"}}\n{%- endif %}\n{{- "Cutting Knowledge Date: December 2023\\n" }}\n{{- "Today Date: " + date_string + "\\n\\n" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- "<|eot_id|>" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|start_header_id|>user<|end_header_id|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot_id|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\\n\\n\'+ message[\'content\'] | trim + \'<|eot_id|>\' }}\n    {%- elif \'tool_calls\' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception("This model only supports single tool-calls at once!") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- "<|python_tag|>" + tool_call.name + ".call(" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + \'="\' + arg_val + \'"\' }}\n                {%- if not loop.last %}\n                    {{- ", " }}\n                {%- endif %}\n                {%- endfor %}\n            {{- ")" }}\n        {%- else  %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- \'{"name": "\' + tool_call.name + \'", \' }}\n            {{- \'"parameters": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- "}" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we\'re in ipython mode #}\n            {{- "<|eom_id|>" }}\n        {%- else %}\n            {{- "<|eot_id|>" }}\n        {%- endif %}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' }}\n{%- endif %}\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.value_length': '128', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.padding_token_id': '128004', 'general.basename': 'Llama-3.3', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '64', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Llama 3.3 70B Instruct', 'general.organization': 'Meta Llama', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '70B', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8'}
Available chat formats from metadata: chat_template.default
.
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 8192
llama_init_from_model: n_batch       = 512
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 48: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 49: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 50: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 51: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 52: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 53: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 54: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 55: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 56: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 57: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 58: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 59: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 60: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 61: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 62: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 63: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 64: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 65: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 66: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 67: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 68: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 69: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 70: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 71: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 72: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 73: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 74: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 75: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 76: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 77: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 78: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 79: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
.llama_kv_cache_init:      CUDA0 KV buffer size =  2560.00 MiB
llama_init_from_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     0.49 MiB
.llama_init_from_model:      CUDA0 compute buffer size =   266.50 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    32.01 MiB
llama_init_from_model: graph nodes  = 2247
llama_init_from_model: graph splits = 2
CUDA : ARCHS = 900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 
Model metadata: {'tokenizer.chat_template': '{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = "26 Jul 2024" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- "<|start_header_id|>system<|end_header_id|>\\n\\n" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- "Environment: ipython\\n" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- "Tools: " + builtin_tools | reject(\'equalto\', \'code_interpreter\') | join(", ") + "\\n\\n"}}\n{%- endif %}\n{{- "Cutting Knowledge Date: December 2023\\n" }}\n{{- "Today Date: " + date_string + "\\n\\n" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- "<|eot_id|>" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|start_header_id|>user<|end_header_id|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot_id|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\\n\\n\'+ message[\'content\'] | trim + \'<|eot_id|>\' }}\n    {%- elif \'tool_calls\' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception("This model only supports single tool-calls at once!") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- "<|python_tag|>" + tool_call.name + ".call(" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + \'="\' + arg_val + \'"\' }}\n                {%- if not loop.last %}\n                    {{- ", " }}\n                {%- endif %}\n                {%- endfor %}\n            {{- ")" }}\n        {%- else  %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- \'{"name": "\' + tool_call.name + \'", \' }}\n            {{- \'"parameters": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- "}" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we\'re in ipython mode #}\n            {{- "<|eom_id|>" }}\n        {%- else %}\n            {{- "<|eot_id|>" }}\n        {%- endif %}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' }}\n{%- endif %}\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.value_length': '128', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.padding_token_id': '128004', 'general.basename': 'Llama-3.3', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '64', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Llama 3.3 70B Instruct', 'general.organization': 'Meta Llama', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '70B', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8'}
Available chat formats from metadata: chat_template.default
..............
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 8192
llama_init_from_model: n_ctx_per_seq = 8192
llama_init_from_model: n_batch       = 512
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 500000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 48: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 49: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 50: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 51: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 52: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 53: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 54: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 55: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 56: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 57: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 58: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 59: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 60: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 61: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 62: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 63: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 64: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 65: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 66: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 67: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 68: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 69: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 70: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 71: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 72: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 73: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 74: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 75: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 76: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 77: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 78: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init: layer 79: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024
llama_kv_cache_init:      CUDA0 KV buffer size =  2560.00 MiB
llama_init_from_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     0.49 MiB
llama_init_from_model:      CUDA0 compute buffer size =   266.50 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    32.01 MiB
llama_init_from_model: graph nodes  = 2247
llama_init_from_model: graph splits = 2
CUDA : ARCHS = 900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 
Model metadata: {'tokenizer.chat_template': '{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = "26 Jul 2024" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- "<|start_header_id|>system<|end_header_id|>\\n\\n" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- "Environment: ipython\\n" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- "Tools: " + builtin_tools | reject(\'equalto\', \'code_interpreter\') | join(", ") + "\\n\\n"}}\n{%- endif %}\n{{- "Cutting Knowledge Date: December 2023\\n" }}\n{{- "Today Date: " + date_string + "\\n\\n" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- "<|eot_id|>" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|start_header_id|>user<|end_header_id|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot_id|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\\n\\n\'+ message[\'content\'] | trim + \'<|eot_id|>\' }}\n    {%- elif \'tool_calls\' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception("This model only supports single tool-calls at once!") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- "<|python_tag|>" + tool_call.name + ".call(" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + \'="\' + arg_val + \'"\' }}\n                {%- if not loop.last %}\n                    {{- ", " }}\n                {%- endif %}\n                {%- endfor %}\n            {{- ")" }}\n        {%- else  %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- \'{"name": "\' + tool_call.name + \'", \' }}\n            {{- \'"parameters": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- "}" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we\'re in ipython mode #}\n            {{- "<|eom_id|>" }}\n        {%- else %}\n            {{- "<|eot_id|>" }}\n        {%- endif %}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' }}\n{%- endif %}\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.value_length': '128', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.padding_token_id': '128004', 'general.basename': 'Llama-3.3', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '64', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Llama 3.3 70B Instruct', 'general.organization': 'Meta Llama', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '70B', 'llama.embedding_length': '8192', 'llama.feed_forward_length': '28672', 'llama.block_count': '80', 'llama.attention.head_count_kv': '8'}
Available chat formats from metadata: chat_template.default
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    3400.24 ms /  4083 tokens (    0.83 ms per token,  1200.80 tokens per second)
llama_perf_context_print:        eval time =    1964.40 ms /    70 runs   (   28.06 ms per token,    35.63 tokens per second)
llama_perf_context_print:       total time =    5454.08 ms /  4153 tokens
Llama.generate: 3884 prefix-match hit, remaining 590 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    3418.67 ms /  4057 tokens (    0.84 ms per token,  1186.72 tokens per second)
llama_perf_context_print:        eval time =    3571.68 ms /   128 runs   (   27.90 ms per token,    35.84 tokens per second)
llama_perf_context_print:       total time =    7157.39 ms /  4185 tokens
Llama.generate: 3884 prefix-match hit, remaining 864 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    3839.18 ms /  4573 tokens (    0.84 ms per token,  1191.14 tokens per second)
llama_perf_context_print:        eval time =    3472.57 ms /   124 runs   (   28.00 ms per token,    35.71 tokens per second)
llama_perf_context_print:       total time =    7470.49 ms /  4697 tokens
Llama.generate: 3884 prefix-match hit, remaining 347 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    3931.29 ms /  4663 tokens (    0.84 ms per token,  1186.13 tokens per second)
llama_perf_context_print:        eval time =    4002.32 ms /   143 runs   (   27.99 ms per token,    35.73 tokens per second)
llama_perf_context_print:       total time =    8120.23 ms /  4806 tokens
Llama.generate: 3883 prefix-match hit, remaining 2527 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     547.62 ms /   590 tokens (    0.93 ms per token,  1077.38 tokens per second)
llama_perf_context_print:        eval time =    2624.21 ms /    94 runs   (   27.92 ms per token,    35.82 tokens per second)
llama_perf_context_print:       total time =    3296.57 ms /   684 tokens
Llama.generate: 3883 prefix-match hit, remaining 798 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     790.30 ms /   864 tokens (    0.91 ms per token,  1093.25 tokens per second)
llama_perf_context_print:        eval time =    3381.55 ms /   121 runs   (   27.95 ms per token,    35.78 tokens per second)
llama_perf_context_print:       total time =    4332.95 ms /   985 tokens
Llama.generate: 3884 prefix-match hit, remaining 656 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     346.88 ms /   347 tokens (    1.00 ms per token,  1000.33 tokens per second)
llama_perf_context_print:        eval time =    3982.85 ms /   143 runs   (   27.85 ms per token,    35.90 tokens per second)
llama_perf_context_print:       total time =    4517.67 ms /   490 tokens
Llama.generate: 3884 prefix-match hit, remaining 498 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     723.21 ms /   798 tokens (    0.91 ms per token,  1103.41 tokens per second)
llama_perf_context_print:        eval time =    2741.05 ms /    98 runs   (   27.97 ms per token,    35.75 tokens per second)
llama_perf_context_print:       total time =    3593.85 ms /   896 tokens
Llama.generate: 3883 prefix-match hit, remaining 264 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    2221.32 ms /  2527 tokens (    0.88 ms per token,  1137.61 tokens per second)
llama_perf_context_print:        eval time =    4196.41 ms /   148 runs   (   28.35 ms per token,    35.27 tokens per second)
llama_perf_context_print:       total time =    6614.65 ms /  2675 tokens
Llama.generate: 3883 prefix-match hit, remaining 431 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     614.29 ms /   656 tokens (    0.94 ms per token,  1067.90 tokens per second)
llama_perf_context_print:        eval time =    4689.44 ms /   168 runs   (   27.91 ms per token,    35.83 tokens per second)
llama_perf_context_print:       total time =    5532.04 ms /   824 tokens
Llama.generate: 3883 prefix-match hit, remaining 168 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     271.64 ms /   264 tokens (    1.03 ms per token,   971.88 tokens per second)
llama_perf_context_print:        eval time =    3314.01 ms /   119 runs   (   27.85 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    3742.88 ms /   383 tokens
Llama.generate: 3884 prefix-match hit, remaining 135 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     405.52 ms /   431 tokens (    0.94 ms per token,  1062.84 tokens per second)
llama_perf_context_print:        eval time =    2512.05 ms /    90 runs   (   27.91 ms per token,    35.83 tokens per second)
llama_perf_context_print:       total time =    3035.59 ms /   521 tokens
Llama.generate: 3884 prefix-match hit, remaining 1054 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     447.17 ms /   498 tokens (    0.90 ms per token,  1113.68 tokens per second)
llama_perf_context_print:        eval time =    5489.84 ms /   197 runs   (   27.87 ms per token,    35.88 tokens per second)
llama_perf_context_print:       total time =    6201.57 ms /   695 tokens
Llama.generate: 3884 prefix-match hit, remaining 349 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     192.11 ms /   168 tokens (    1.14 ms per token,   874.49 tokens per second)
llama_perf_context_print:        eval time =    1337.26 ms /    48 runs   (   27.86 ms per token,    35.89 tokens per second)
llama_perf_context_print:       total time =    1593.96 ms /   216 tokens
Llama.generate: 3883 prefix-match hit, remaining 210 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     169.77 ms /   135 tokens (    1.26 ms per token,   795.17 tokens per second)
llama_perf_context_print:        eval time =    1893.02 ms /    68 runs   (   27.84 ms per token,    35.92 tokens per second)
llama_perf_context_print:       total time =    2152.65 ms /   203 tokens
Llama.generate: 3884 prefix-match hit, remaining 138 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     347.48 ms /   349 tokens (    1.00 ms per token,  1004.38 tokens per second)
llama_perf_context_print:        eval time =    2781.07 ms /   100 runs   (   27.81 ms per token,    35.96 tokens per second)
llama_perf_context_print:       total time =    3258.29 ms /   449 tokens
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     219.40 ms /   210 tokens (    1.04 ms per token,   957.16 tokens per second)
llama_perf_context_print:        eval time =    2534.10 ms /    91 runs   (   27.85 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    2873.11 ms /   301 tokens
Llama.generate: 3884 prefix-match hit, remaining 689 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     170.69 ms /   138 tokens (    1.24 ms per token,   808.47 tokens per second)
llama_perf_context_print:        eval time =    2029.31 ms /    73 runs   (   27.80 ms per token,    35.97 tokens per second)
llama_perf_context_print:       total time =    2296.08 ms /   211 tokens
Llama.generate: 3883 prefix-match hit, remaining 1034 prompt tokens to eval
Llama.generate: 3884 prefix-match hit, remaining 3805 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     955.91 ms /  1054 tokens (    0.91 ms per token,  1102.61 tokens per second)
llama_perf_context_print:        eval time =    5374.61 ms /   192 runs   (   27.99 ms per token,    35.72 tokens per second)
llama_perf_context_print:       total time =    6588.80 ms /  1246 tokens
Llama.generate: 3883 prefix-match hit, remaining 1169 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     941.48 ms /  1034 tokens (    0.91 ms per token,  1098.27 tokens per second)
llama_perf_context_print:        eval time =    2908.78 ms /   104 runs   (   27.97 ms per token,    35.75 tokens per second)
llama_perf_context_print:       total time =    3987.25 ms /  1138 tokens
Llama.generate: 3883 prefix-match hit, remaining 2558 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     635.29 ms /   689 tokens (    0.92 ms per token,  1084.54 tokens per second)
llama_perf_context_print:        eval time =    3491.80 ms /   125 runs   (   27.93 ms per token,    35.80 tokens per second)
llama_perf_context_print:       total time =    4291.30 ms /   814 tokens
Llama.generate: 3884 prefix-match hit, remaining 938 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1057.27 ms /  1169 tokens (    0.90 ms per token,  1105.68 tokens per second)
llama_perf_context_print:        eval time =    3838.84 ms /   137 runs   (   28.02 ms per token,    35.69 tokens per second)
llama_perf_context_print:       total time =    5077.05 ms /  1306 tokens
Llama.generate: 3883 prefix-match hit, remaining 271 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    3365.13 ms /  3805 tokens (    0.88 ms per token,  1130.71 tokens per second)
llama_perf_context_print:        eval time =    4989.20 ms /   174 runs   (   28.67 ms per token,    34.88 tokens per second)
llama_perf_context_print:       total time =    8589.18 ms /  3979 tokens
Llama.generate: 3883 prefix-match hit, remaining 1586 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     845.89 ms /   938 tokens (    0.90 ms per token,  1108.89 tokens per second)
llama_perf_context_print:        eval time =    4419.25 ms /   158 runs   (   27.97 ms per token,    35.75 tokens per second)
llama_perf_context_print:       total time =    5473.39 ms /  1096 tokens
Llama.generate: 3884 prefix-match hit, remaining 1893 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     279.67 ms /   271 tokens (    1.03 ms per token,   969.02 tokens per second)
llama_perf_context_print:        eval time =    4032.93 ms /   145 runs   (   27.81 ms per token,    35.95 tokens per second)
llama_perf_context_print:       total time =    4503.61 ms /   416 tokens
Llama.generate: 3884 prefix-match hit, remaining 1649 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    2220.74 ms /  2558 tokens (    0.87 ms per token,  1151.87 tokens per second)
llama_perf_context_print:        eval time =    6515.52 ms /   230 runs   (   28.33 ms per token,    35.30 tokens per second)
llama_perf_context_print:       total time =    9057.33 ms /  2788 tokens
Llama.generate: 3884 prefix-match hit, remaining 370 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    1422.59 ms /  1586 tokens (    0.90 ms per token,  1114.87 tokens per second)
llama_perf_context_print:        eval time =    3768.94 ms /   134 runs   (   28.13 ms per token,    35.55 tokens per second)
llama_perf_context_print:       total time =    5369.57 ms /  1720 tokens
Llama.generate: 3883 prefix-match hit, remaining 1387 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     352.77 ms /   370 tokens (    0.95 ms per token,  1048.84 tokens per second)
llama_perf_context_print:        eval time =    3840.49 ms /   138 runs   (   27.83 ms per token,    35.93 tokens per second)
llama_perf_context_print:       total time =    4375.26 ms /   508 tokens
Llama.generate: 3884 prefix-match hit, remaining 949 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1678.51 ms /  1893 tokens (    0.89 ms per token,  1127.78 tokens per second)
llama_perf_context_print:        eval time =    6338.52 ms /   225 runs   (   28.17 ms per token,    35.50 tokens per second)
llama_perf_context_print:       total time =    8322.14 ms /  2118 tokens
Llama.generate: 3884 prefix-match hit, remaining 1500 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    1237.12 ms /  1387 tokens (    0.89 ms per token,  1121.15 tokens per second)
llama_perf_context_print:        eval time =    4017.66 ms /   143 runs   (   28.10 ms per token,    35.59 tokens per second)
llama_perf_context_print:       total time =    5444.40 ms /  1530 tokens
Llama.generate: 3883 prefix-match hit, remaining 337 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1478.06 ms /  1649 tokens (    0.90 ms per token,  1115.66 tokens per second)
llama_perf_context_print:        eval time =    6103.56 ms /   217 runs   (   28.13 ms per token,    35.55 tokens per second)
llama_perf_context_print:       total time =    7876.89 ms /  1866 tokens
Llama.generate: 3884 prefix-match hit, remaining 213 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1327.58 ms /  1500 tokens (    0.89 ms per token,  1129.88 tokens per second)
llama_perf_context_print:        eval time =    2809.66 ms /   100 runs   (   28.10 ms per token,    35.59 tokens per second)
llama_perf_context_print:       total time =    4268.44 ms /  1600 tokens
Llama.generate: 3884 prefix-match hit, remaining 1570 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     220.04 ms /   213 tokens (    1.03 ms per token,   967.99 tokens per second)
llama_perf_context_print:        eval time =    2087.26 ms /    75 runs   (   27.83 ms per token,    35.93 tokens per second)
llama_perf_context_print:       total time =    2405.37 ms /   288 tokens
Llama.generate: 3884 prefix-match hit, remaining 919 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     348.80 ms /   337 tokens (    1.04 ms per token,   966.18 tokens per second)
llama_perf_context_print:        eval time =    3564.13 ms /   128 runs   (   27.84 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    4082.12 ms /   465 tokens
Llama.generate: 3883 prefix-match hit, remaining 148 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     842.01 ms /   949 tokens (    0.89 ms per token,  1127.06 tokens per second)
llama_perf_context_print:        eval time =    5757.78 ms /   206 runs   (   27.95 ms per token,    35.78 tokens per second)
llama_perf_context_print:       total time =    6880.09 ms /  1155 tokens
Llama.generate: 3883 prefix-match hit, remaining 1067 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     171.82 ms /   148 tokens (    1.16 ms per token,   861.37 tokens per second)
llama_perf_context_print:        eval time =    1839.21 ms /    66 runs   (   27.87 ms per token,    35.89 tokens per second)
llama_perf_context_print:       total time =    2098.12 ms /   214 tokens
Llama.generate: 3884 prefix-match hit, remaining 2037 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     842.55 ms /   919 tokens (    0.92 ms per token,  1090.74 tokens per second)
llama_perf_context_print:        eval time =    3775.32 ms /   135 runs   (   27.97 ms per token,    35.76 tokens per second)
llama_perf_context_print:       total time =    4795.20 ms /  1054 tokens
Llama.generate: 3884 prefix-match hit, remaining 260 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1405.25 ms /  1570 tokens (    0.90 ms per token,  1117.24 tokens per second)
llama_perf_context_print:        eval time =    5309.18 ms /   189 runs   (   28.09 ms per token,    35.60 tokens per second)
llama_perf_context_print:       total time =    6966.50 ms /  1759 tokens
Llama.generate: 3884 prefix-match hit, remaining 1356 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    1779.10 ms /  2037 tokens (    0.87 ms per token,  1144.96 tokens per second)
llama_perf_context_print:        eval time =    2879.76 ms /   102 runs   (   28.23 ms per token,    35.42 tokens per second)
llama_perf_context_print:       total time =    4793.12 ms /  2139 tokens
Llama.generate: 3884 prefix-match hit, remaining 2210 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     271.53 ms /   260 tokens (    1.04 ms per token,   957.55 tokens per second)
llama_perf_context_print:        eval time =    3447.92 ms /   124 runs   (   27.81 ms per token,    35.96 tokens per second)
llama_perf_context_print:       total time =    3881.29 ms /   384 tokens
Llama.generate: 3883 prefix-match hit, remaining 898 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     964.50 ms /  1067 tokens (    0.90 ms per token,  1106.27 tokens per second)
llama_perf_context_print:        eval time =    6013.40 ms /   215 runs   (   27.97 ms per token,    35.75 tokens per second)
llama_perf_context_print:       total time =    7271.73 ms /  1282 tokens
Llama.generate: 3883 prefix-match hit, remaining 752 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1204.43 ms /  1356 tokens (    0.89 ms per token,  1125.84 tokens per second)
llama_perf_context_print:        eval time =    4011.74 ms /   143 runs   (   28.05 ms per token,    35.65 tokens per second)
llama_perf_context_print:       total time =    5403.64 ms /  1499 tokens
Llama.generate: 3884 prefix-match hit, remaining 2061 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    1969.60 ms /  2210 tokens (    0.89 ms per token,  1122.06 tokens per second)
llama_perf_context_print:        eval time =    3989.86 ms /   141 runs   (   28.30 ms per token,    35.34 tokens per second)
llama_perf_context_print:       total time =    6147.19 ms /  2351 tokens
Llama.generate: 3883 prefix-match hit, remaining 2916 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     682.45 ms /   752 tokens (    0.91 ms per token,  1101.92 tokens per second)
llama_perf_context_print:        eval time =    4379.50 ms /   157 runs   (   27.89 ms per token,    35.85 tokens per second)
llama_perf_context_print:       total time =    5270.67 ms /   909 tokens
Llama.generate: 3884 prefix-match hit, remaining 2238 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     841.17 ms /   898 tokens (    0.94 ms per token,  1067.56 tokens per second)
llama_perf_context_print:        eval time =    5004.70 ms /   179 runs   (   27.96 ms per token,    35.77 tokens per second)
llama_perf_context_print:       total time =    6085.37 ms /  1077 tokens
Llama.generate: 3883 prefix-match hit, remaining 381 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     352.76 ms /   381 tokens (    0.93 ms per token,  1080.05 tokens per second)
llama_perf_context_print:        eval time =    2281.02 ms /    82 runs   (   27.82 ms per token,    35.95 tokens per second)
llama_perf_context_print:       total time =    2740.54 ms /   463 tokens
Llama.generate: 3883 prefix-match hit, remaining 71 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     102.78 ms /    71 tokens (    1.45 ms per token,   690.82 tokens per second)
llama_perf_context_print:        eval time =     558.04 ms /    20 runs   (   27.90 ms per token,    35.84 tokens per second)
llama_perf_context_print:       total time =     690.96 ms /    91 tokens
Llama.generate: 3884 prefix-match hit, remaining 787 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1833.74 ms /  2061 tokens (    0.89 ms per token,  1123.93 tokens per second)
llama_perf_context_print:        eval time =    5609.01 ms /   199 runs   (   28.19 ms per token,    35.48 tokens per second)
llama_perf_context_print:       total time =    7708.96 ms /  2260 tokens
Llama.generate: 3884 prefix-match hit, remaining 195 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    2584.22 ms /  2916 tokens (    0.89 ms per token,  1128.39 tokens per second)
llama_perf_context_print:        eval time =    4522.93 ms /   159 runs   (   28.45 ms per token,    35.15 tokens per second)
llama_perf_context_print:       total time =    7319.59 ms /  3075 tokens
Llama.generate: 3883 prefix-match hit, remaining 381 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     723.16 ms /   787 tokens (    0.92 ms per token,  1088.27 tokens per second)
llama_perf_context_print:        eval time =    3935.81 ms /   141 runs   (   27.91 ms per token,    35.82 tokens per second)
llama_perf_context_print:       total time =    4845.00 ms /   928 tokens
Llama.generate: 3883 prefix-match hit, remaining 1915 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     218.31 ms /   195 tokens (    1.12 ms per token,   893.22 tokens per second)
llama_perf_context_print:        eval time =    3393.79 ms /   122 runs   (   27.82 ms per token,    35.95 tokens per second)
llama_perf_context_print:       total time =    3770.45 ms /   317 tokens
Llama.generate: 3884 prefix-match hit, remaining 406 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1971.62 ms /  2238 tokens (    0.88 ms per token,  1135.11 tokens per second)
llama_perf_context_print:        eval time =    7540.58 ms /   267 runs   (   28.24 ms per token,    35.41 tokens per second)
llama_perf_context_print:       total time =    9886.39 ms /  2505 tokens
Llama.generate: 3884 prefix-match hit, remaining 1198 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     354.00 ms /   381 tokens (    0.93 ms per token,  1076.27 tokens per second)
llama_perf_context_print:        eval time =    2900.28 ms /   104 runs   (   27.89 ms per token,    35.86 tokens per second)
llama_perf_context_print:       total time =    3390.96 ms /   485 tokens
Llama.generate: 3883 prefix-match hit, remaining 548 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     401.82 ms /   406 tokens (    0.99 ms per token,  1010.41 tokens per second)
llama_perf_context_print:        eval time =    3481.24 ms /   125 runs   (   27.85 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    4045.57 ms /   531 tokens
Llama.generate: 3884 prefix-match hit, remaining 195 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     520.65 ms /   548 tokens (    0.95 ms per token,  1052.53 tokens per second)
llama_perf_context_print:        eval time =    2537.95 ms /    91 runs   (   27.89 ms per token,    35.86 tokens per second)
llama_perf_context_print:       total time =    3178.97 ms /   639 tokens
Llama.generate: 3884 prefix-match hit, remaining 1012 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     216.56 ms /   195 tokens (    1.11 ms per token,   900.43 tokens per second)
llama_perf_context_print:        eval time =    1785.31 ms /    64 runs   (   27.90 ms per token,    35.85 tokens per second)
llama_perf_context_print:       total time =    2085.40 ms /   259 tokens
Llama.generate: 3884 prefix-match hit, remaining 845 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1076.47 ms /  1198 tokens (    0.90 ms per token,  1112.90 tokens per second)
llama_perf_context_print:        eval time =    4679.15 ms /   167 runs   (   28.02 ms per token,    35.69 tokens per second)
llama_perf_context_print:       total time =    5978.88 ms /  1365 tokens
Llama.generate: 3884 prefix-match hit, remaining 229 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1684.49 ms /  1915 tokens (    0.88 ms per token,  1136.85 tokens per second)
llama_perf_context_print:        eval time =    5074.55 ms /   180 runs   (   28.19 ms per token,    35.47 tokens per second)
llama_perf_context_print:       total time =    7000.15 ms /  2095 tokens
Llama.generate: 3883 prefix-match hit, remaining 3425 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     239.14 ms /   229 tokens (    1.04 ms per token,   957.60 tokens per second)
llama_perf_context_print:        eval time =    1503.49 ms /    54 runs   (   27.84 ms per token,    35.92 tokens per second)
llama_perf_context_print:       total time =    1813.99 ms /   283 tokens
Llama.generate: 3883 prefix-match hit, remaining 796 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     888.84 ms /  1012 tokens (    0.88 ms per token,  1138.56 tokens per second)
llama_perf_context_print:        eval time =    3555.98 ms /   127 runs   (   28.00 ms per token,    35.71 tokens per second)
llama_perf_context_print:       total time =    4612.64 ms /  1139 tokens
Llama.generate: 3884 prefix-match hit, remaining 218 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     760.37 ms /   845 tokens (    0.90 ms per token,  1111.30 tokens per second)
llama_perf_context_print:        eval time =    4160.41 ms /   149 runs   (   27.92 ms per token,    35.81 tokens per second)
llama_perf_context_print:       total time =    5116.55 ms /   994 tokens
Llama.generate: 3884 prefix-match hit, remaining 1102 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     220.45 ms /   218 tokens (    1.01 ms per token,   988.88 tokens per second)
llama_perf_context_print:        eval time =    2422.87 ms /    87 runs   (   27.85 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    2757.38 ms /   305 tokens
Llama.generate: 3883 prefix-match hit, remaining 182 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     723.58 ms /   796 tokens (    0.91 ms per token,  1100.08 tokens per second)
llama_perf_context_print:        eval time =    3877.18 ms /   139 runs   (   27.89 ms per token,    35.85 tokens per second)
llama_perf_context_print:       total time =    4783.78 ms /   935 tokens
Llama.generate: 3883 prefix-match hit, remaining 352 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     193.90 ms /   182 tokens (    1.07 ms per token,   938.64 tokens per second)
llama_perf_context_print:        eval time =    3369.55 ms /   121 runs   (   27.85 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    3722.94 ms /   303 tokens
Llama.generate: 3883 prefix-match hit, remaining 725 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    3033.53 ms /  3425 tokens (    0.89 ms per token,  1129.05 tokens per second)
llama_perf_context_print:        eval time =    6024.25 ms /   211 runs   (   28.55 ms per token,    35.03 tokens per second)
llama_perf_context_print:       total time =    9343.20 ms /  3636 tokens
Llama.generate: 3884 prefix-match hit, remaining 208 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     989.14 ms /  1102 tokens (    0.90 ms per token,  1114.10 tokens per second)
llama_perf_context_print:        eval time =    4758.06 ms /   170 runs   (   27.99 ms per token,    35.73 tokens per second)
llama_perf_context_print:       total time =    5971.88 ms /  1272 tokens
Llama.generate: 3884 prefix-match hit, remaining 505 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     347.89 ms /   352 tokens (    0.99 ms per token,  1011.81 tokens per second)
llama_perf_context_print:        eval time =    4951.93 ms /   178 runs   (   27.82 ms per token,    35.95 tokens per second)
llama_perf_context_print:       total time =    5537.88 ms /   530 tokens
Llama.generate: 3884 prefix-match hit, remaining 856 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     661.94 ms /   725 tokens (    0.91 ms per token,  1095.27 tokens per second)
llama_perf_context_print:        eval time =    2877.15 ms /   103 runs   (   27.93 ms per token,    35.80 tokens per second)
llama_perf_context_print:       total time =    3675.12 ms /   828 tokens
Llama.generate: 3884 prefix-match hit, remaining 999 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     217.90 ms /   208 tokens (    1.05 ms per token,   954.56 tokens per second)
llama_perf_context_print:        eval time =    3339.25 ms /   120 runs   (   27.83 ms per token,    35.94 tokens per second)
llama_perf_context_print:       total time =    3713.86 ms /   328 tokens
Llama.generate: 3884 prefix-match hit, remaining 121 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     447.67 ms /   505 tokens (    0.89 ms per token,  1128.06 tokens per second)
llama_perf_context_print:        eval time =    2591.01 ms /    93 runs   (   27.86 ms per token,    35.89 tokens per second)
llama_perf_context_print:       total time =    3159.45 ms /   598 tokens
Llama.generate: 3884 prefix-match hit, remaining 297 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     147.94 ms /   121 tokens (    1.22 ms per token,   817.87 tokens per second)
llama_perf_context_print:        eval time =    1111.47 ms /    40 runs   (   27.79 ms per token,    35.99 tokens per second)
llama_perf_context_print:       total time =    1313.54 ms /   161 tokens
Llama.generate: 3884 prefix-match hit, remaining 1173 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     791.42 ms /   856 tokens (    0.92 ms per token,  1081.60 tokens per second)
llama_perf_context_print:        eval time =    3180.05 ms /   114 runs   (   27.90 ms per token,    35.85 tokens per second)
llama_perf_context_print:       total time =    4120.75 ms /   970 tokens
Llama.generate: 3884 prefix-match hit, remaining 2241 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     316.27 ms /   297 tokens (    1.06 ms per token,   939.06 tokens per second)
llama_perf_context_print:        eval time =    2668.96 ms /    96 runs   (   27.80 ms per token,    35.97 tokens per second)
llama_perf_context_print:       total time =    3109.28 ms /   393 tokens
Llama.generate: 3884 prefix-match hit, remaining 2914 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     887.28 ms /   999 tokens (    0.89 ms per token,  1125.91 tokens per second)
llama_perf_context_print:        eval time =    4114.11 ms /   147 runs   (   27.99 ms per token,    35.73 tokens per second)
llama_perf_context_print:       total time =    5196.60 ms /  1146 tokens
Llama.generate: 3884 prefix-match hit, remaining 679 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1056.74 ms /  1173 tokens (    0.90 ms per token,  1110.02 tokens per second)
llama_perf_context_print:        eval time =    3336.43 ms /   119 runs   (   28.04 ms per token,    35.67 tokens per second)
llama_perf_context_print:       total time =    4549.57 ms /  1292 tokens
Llama.generate: 3884 prefix-match hit, remaining 1380 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     635.01 ms /   679 tokens (    0.94 ms per token,  1069.28 tokens per second)
llama_perf_context_print:        eval time =    2599.87 ms /    93 runs   (   27.96 ms per token,    35.77 tokens per second)
llama_perf_context_print:       total time =    3357.73 ms /   772 tokens
Llama.generate: 3884 prefix-match hit, remaining 2072 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1235.31 ms /  1380 tokens (    0.90 ms per token,  1117.13 tokens per second)
llama_perf_context_print:        eval time =    4403.97 ms /   157 runs   (   28.05 ms per token,    35.65 tokens per second)
llama_perf_context_print:       total time =    5847.24 ms /  1537 tokens
Llama.generate: 3883 prefix-match hit, remaining 516 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1993.81 ms /  2241 tokens (    0.89 ms per token,  1123.98 tokens per second)
llama_perf_context_print:        eval time =    6609.79 ms /   234 runs   (   28.25 ms per token,    35.40 tokens per second)
llama_perf_context_print:       total time =    8926.48 ms /  2475 tokens
Llama.generate: 3883 prefix-match hit, remaining 1444 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    2578.87 ms /  2914 tokens (    0.88 ms per token,  1129.95 tokens per second)
llama_perf_context_print:        eval time =    6310.58 ms /   222 runs   (   28.43 ms per token,    35.18 tokens per second)
llama_perf_context_print:       total time =    9190.34 ms /  3136 tokens
Llama.generate: 3885 prefix-match hit, remaining 897 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    1842.90 ms /  2072 tokens (    0.89 ms per token,  1124.31 tokens per second)
llama_perf_context_print:        eval time =    3783.84 ms /   134 runs   (   28.24 ms per token,    35.41 tokens per second)
llama_perf_context_print:       total time =    5804.28 ms /  2206 tokens
Llama.generate: 3884 prefix-match hit, remaining 710 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     497.92 ms /   516 tokens (    0.96 ms per token,  1036.32 tokens per second)
llama_perf_context_print:        eval time =    3287.98 ms /   118 runs   (   27.86 ms per token,    35.89 tokens per second)
llama_perf_context_print:       total time =    3940.70 ms /   634 tokens
Llama.generate: 3883 prefix-match hit, remaining 3540 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1287.52 ms /  1444 tokens (    0.89 ms per token,  1121.53 tokens per second)
llama_perf_context_print:        eval time =    3844.78 ms /   137 runs   (   28.06 ms per token,    35.63 tokens per second)
llama_perf_context_print:       total time =    5312.87 ms /  1581 tokens
Llama.generate: 3883 prefix-match hit, remaining 246 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     661.09 ms /   710 tokens (    0.93 ms per token,  1073.99 tokens per second)
llama_perf_context_print:        eval time =    3020.26 ms /   108 runs   (   27.97 ms per token,    35.76 tokens per second)
llama_perf_context_print:       total time =    3823.48 ms /   818 tokens
Llama.generate: 3884 prefix-match hit, remaining 252 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     840.27 ms /   897 tokens (    0.94 ms per token,  1067.51 tokens per second)
llama_perf_context_print:        eval time =    3828.92 ms /   137 runs   (   27.95 ms per token,    35.78 tokens per second)
llama_perf_context_print:       total time =    4848.69 ms /  1034 tokens
Llama.generate: 3884 prefix-match hit, remaining 837 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     241.90 ms /   246 tokens (    0.98 ms per token,  1016.95 tokens per second)
llama_perf_context_print:        eval time =    2501.67 ms /    90 runs   (   27.80 ms per token,    35.98 tokens per second)
llama_perf_context_print:       total time =    2861.87 ms /   336 tokens
Llama.generate: 3884 prefix-match hit, remaining 1027 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     242.95 ms /   252 tokens (    0.96 ms per token,  1037.27 tokens per second)
llama_perf_context_print:        eval time =    2533.12 ms /    91 runs   (   27.84 ms per token,    35.92 tokens per second)
llama_perf_context_print:       total time =    2895.65 ms /   343 tokens
Llama.generate: 3885 prefix-match hit, remaining 1494 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     761.27 ms /   837 tokens (    0.91 ms per token,  1099.48 tokens per second)
llama_perf_context_print:        eval time =    2706.67 ms /    97 runs   (   27.90 ms per token,    35.84 tokens per second)
llama_perf_context_print:       total time =    3594.13 ms /   934 tokens
Llama.generate: 3884 prefix-match hit, remaining 1905 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    1329.89 ms /  1494 tokens (    0.89 ms per token,  1123.40 tokens per second)
llama_perf_context_print:        eval time =    2726.05 ms /    97 runs   (   28.10 ms per token,    35.58 tokens per second)
llama_perf_context_print:       total time =    4183.48 ms /  1591 tokens
Llama.generate: 3884 prefix-match hit, remaining 1624 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    3127.59 ms /  3540 tokens (    0.88 ms per token,  1131.86 tokens per second)
llama_perf_context_print:        eval time =    6688.02 ms /   234 runs   (   28.58 ms per token,    34.99 tokens per second)
llama_perf_context_print:       total time =   10137.35 ms /  3774 tokens
Llama.generate: 3884 prefix-match hit, remaining 1689 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     929.90 ms /  1027 tokens (    0.91 ms per token,  1104.41 tokens per second)
llama_perf_context_print:        eval time =    4555.33 ms /   163 runs   (   27.95 ms per token,    35.78 tokens per second)
llama_perf_context_print:       total time =    5704.74 ms /  1190 tokens
Llama.generate: 3884 prefix-match hit, remaining 266 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     279.41 ms /   266 tokens (    1.05 ms per token,   952.02 tokens per second)
llama_perf_context_print:        eval time =    2280.50 ms /    82 runs   (   27.81 ms per token,    35.96 tokens per second)
llama_perf_context_print:       total time =    2666.50 ms /   348 tokens
Llama.generate: 3884 prefix-match hit, remaining 272 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1682.52 ms /  1905 tokens (    0.88 ms per token,  1132.23 tokens per second)
llama_perf_context_print:        eval time =    5469.38 ms /   194 runs   (   28.19 ms per token,    35.47 tokens per second)
llama_perf_context_print:       total time =    7410.92 ms /  2099 tokens
Llama.generate: 3884 prefix-match hit, remaining 1834 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    1448.67 ms /  1624 tokens (    0.89 ms per token,  1121.03 tokens per second)
llama_perf_context_print:        eval time =    4843.63 ms /   172 runs   (   28.16 ms per token,    35.51 tokens per second)
llama_perf_context_print:       total time =    6523.26 ms /  1796 tokens
Llama.generate: 3884 prefix-match hit, remaining 701 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     279.42 ms /   272 tokens (    1.03 ms per token,   973.46 tokens per second)
llama_perf_context_print:        eval time =    3334.14 ms /   120 runs   (   27.78 ms per token,    35.99 tokens per second)
llama_perf_context_print:       total time =    3770.84 ms /   392 tokens
Llama.generate: 3884 prefix-match hit, remaining 278 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1502.19 ms /  1689 tokens (    0.89 ms per token,  1124.36 tokens per second)
llama_perf_context_print:        eval time =    4950.94 ms /   176 runs   (   28.13 ms per token,    35.55 tokens per second)
llama_perf_context_print:       total time =    6688.39 ms /  1865 tokens
Llama.generate: 3884 prefix-match hit, remaining 200 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     217.58 ms /   200 tokens (    1.09 ms per token,   919.21 tokens per second)
llama_perf_context_print:        eval time =    2645.21 ms /    95 runs   (   27.84 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    2986.40 ms /   295 tokens
Llama.generate: 3884 prefix-match hit, remaining 1880 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     636.79 ms /   701 tokens (    0.91 ms per token,  1100.84 tokens per second)
llama_perf_context_print:        eval time =    3744.75 ms /   134 runs   (   27.95 ms per token,    35.78 tokens per second)
llama_perf_context_print:       total time =    4559.02 ms /   835 tokens
Llama.generate: 3883 prefix-match hit, remaining 772 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     279.27 ms /   278 tokens (    1.00 ms per token,   995.45 tokens per second)
llama_perf_context_print:        eval time =    3196.98 ms /   115 runs   (   27.80 ms per token,    35.97 tokens per second)
llama_perf_context_print:       total time =    3626.96 ms /   393 tokens
Llama.generate: 3884 prefix-match hit, remaining 192 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1647.21 ms /  1834 tokens (    0.90 ms per token,  1113.40 tokens per second)
llama_perf_context_print:        eval time =    5376.52 ms /   191 runs   (   28.15 ms per token,    35.52 tokens per second)
llama_perf_context_print:       total time =    7278.13 ms /  2025 tokens
Llama.generate: 3884 prefix-match hit, remaining 315 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     193.58 ms /   192 tokens (    1.01 ms per token,   991.85 tokens per second)
llama_perf_context_print:        eval time =    2337.57 ms /    84 runs   (   27.83 ms per token,    35.93 tokens per second)
llama_perf_context_print:       total time =    2640.88 ms /   276 tokens
Llama.generate: 3884 prefix-match hit, remaining 707 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     317.97 ms /   315 tokens (    1.01 ms per token,   990.65 tokens per second)
llama_perf_context_print:        eval time =    2085.92 ms /    75 runs   (   27.81 ms per token,    35.96 tokens per second)
llama_perf_context_print:       total time =    2501.77 ms /   390 tokens
Llama.generate: 3884 prefix-match hit, remaining 601 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =     715.10 ms /   772 tokens (    0.93 ms per token,  1079.57 tokens per second)
llama_perf_context_print:        eval time =    3213.65 ms /   115 runs   (   27.94 ms per token,    35.78 tokens per second)
llama_perf_context_print:       total time =    4080.72 ms /   887 tokens
Llama.generate: 3883 prefix-match hit, remaining 1802 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     561.38 ms /   601 tokens (    0.93 ms per token,  1070.57 tokens per second)
llama_perf_context_print:        eval time =    3147.72 ms /   113 runs   (   27.86 ms per token,    35.90 tokens per second)
llama_perf_context_print:       total time =    3856.15 ms /   714 tokens
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     658.49 ms /   707 tokens (    0.93 ms per token,  1073.66 tokens per second)
llama_perf_context_print:        eval time =    3099.51 ms /   111 runs   (   27.92 ms per token,    35.81 tokens per second)
llama_perf_context_print:       total time =    3903.62 ms /   818 tokens
Llama.generate: 3884 prefix-match hit, remaining 1493 prompt tokens to eval
Llama.generate: 3883 prefix-match hit, remaining 1347 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1678.48 ms /  1880 tokens (    0.89 ms per token,  1120.06 tokens per second)
llama_perf_context_print:        eval time =    5266.98 ms /   187 runs   (   28.17 ms per token,    35.50 tokens per second)
llama_perf_context_print:       total time =    7196.44 ms /  2067 tokens
Llama.generate: 3884 prefix-match hit, remaining 442 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     407.76 ms /   442 tokens (    0.92 ms per token,  1083.97 tokens per second)
llama_perf_context_print:        eval time =    3485.92 ms /   125 runs   (   27.89 ms per token,    35.86 tokens per second)
llama_perf_context_print:       total time =    4057.96 ms /   567 tokens
Llama.generate: 3884 prefix-match hit, remaining 410 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    1613.80 ms /  1802 tokens (    0.90 ms per token,  1116.62 tokens per second)
llama_perf_context_print:        eval time =    6933.78 ms /   246 runs   (   28.19 ms per token,    35.48 tokens per second)
llama_perf_context_print:       total time =    8889.62 ms /  2048 tokens
Llama.generate: 3883 prefix-match hit, remaining 3894 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1202.12 ms /  1347 tokens (    0.89 ms per token,  1120.52 tokens per second)
llama_perf_context_print:        eval time =    5187.46 ms /   185 runs   (   28.04 ms per token,    35.66 tokens per second)
llama_perf_context_print:       total time =    6638.34 ms /  1532 tokens
Llama.generate: 3883 prefix-match hit, remaining 455 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1327.60 ms /  1493 tokens (    0.89 ms per token,  1124.59 tokens per second)
llama_perf_context_print:        eval time =    5250.80 ms /   187 runs   (   28.08 ms per token,    35.61 tokens per second)
llama_perf_context_print:       total time =    6828.95 ms /  1680 tokens
Llama.generate: 3884 prefix-match hit, remaining 1292 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     402.57 ms /   410 tokens (    0.98 ms per token,  1018.46 tokens per second)
llama_perf_context_print:        eval time =    2564.12 ms /    92 runs   (   27.87 ms per token,    35.88 tokens per second)
llama_perf_context_print:       total time =    3086.85 ms /   502 tokens
Llama.generate: 3884 prefix-match hit, remaining 1716 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     439.10 ms /   455 tokens (    0.97 ms per token,  1036.21 tokens per second)
llama_perf_context_print:        eval time =    3733.29 ms /   134 runs   (   27.86 ms per token,    35.89 tokens per second)
llama_perf_context_print:       total time =    4348.69 ms /   589 tokens
Llama.generate: 3883 prefix-match hit, remaining 579 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1163.92 ms /  1292 tokens (    0.90 ms per token,  1110.04 tokens per second)
llama_perf_context_print:        eval time =    3391.09 ms /   121 runs   (   28.03 ms per token,    35.68 tokens per second)
llama_perf_context_print:       total time =    4712.24 ms /  1413 tokens
Llama.generate: 3884 prefix-match hit, remaining 2990 prompt tokens to eval
llama_perf_context_print:        load time =    3400.93 ms
llama_perf_context_print: prompt eval time =    3463.66 ms /  3894 tokens (    0.89 ms per token,  1124.25 tokens per second)
llama_perf_context_print:        eval time =    3384.04 ms /   118 runs   (   28.68 ms per token,    34.87 tokens per second)
llama_perf_context_print:       total time =    7002.78 ms /  4012 tokens
Traceback (most recent call last):
  File "/gpfs/home4/bryanc/multi-modality/histopathology-amc/MLLM_PATH/src/process_reports/process_reports_with_llama_cpp.py", line 100, in <module>
    main()
  File "/gpfs/home4/bryanc/multi-modality/histopathology-amc/MLLM_PATH/src/process_reports/process_reports_with_llama_cpp.py", line 86, in main
    batch_results = process_batch(model, batch_texts, base_prompt, config, args.num_variations)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home4/bryanc/multi-modality/histopathology-amc/MLLM_PATH/src/process_reports/process_reports_with_llama_cpp.py", line 25, in process_batch
    output = model.create_chat_completion(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home4/bryanc/multi-modality/histopathology-amc/venv/lib/python3.12/site-packages/llama_cpp/llama.py", line 2001, in create_chat_completion
    return handler(
           ^^^^^^^^
  File "/gpfs/home4/bryanc/multi-modality/histopathology-amc/venv/lib/python3.12/site-packages/llama_cpp/llama_chat_format.py", line 662, in chat_completion_handler
    completion_or_chunks = llama.create_completion(
                           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home4/bryanc/multi-modality/histopathology-amc/venv/lib/python3.12/site-packages/llama_cpp/llama.py", line 1835, in create_completion
    completion: Completion = next(completion_or_chunks)  # type: ignore
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home4/bryanc/multi-modality/histopathology-amc/venv/lib/python3.12/site-packages/llama_cpp/llama.py", line 1271, in _create_completion
    raise ValueError(
ValueError: Requested tokens (8843) exceed context window of 8192
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     545.70 ms /   579 tokens (    0.94 ms per token,  1061.03 tokens per second)
llama_perf_context_print:        eval time =    1923.23 ms /    69 runs   (   27.87 ms per token,    35.88 tokens per second)
llama_perf_context_print:       total time =    2559.58 ms /   648 tokens
Llama.generate: 3883 prefix-match hit, remaining 450 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1523.33 ms /  1716 tokens (    0.89 ms per token,  1126.48 tokens per second)
llama_perf_context_print:        eval time =    4896.65 ms /   174 runs   (   28.14 ms per token,    35.53 tokens per second)
llama_perf_context_print:       total time =    6651.92 ms /  1890 tokens
Llama.generate: 3884 prefix-match hit, remaining 642 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    2636.00 ms /  2990 tokens (    0.88 ms per token,  1134.29 tokens per second)
llama_perf_context_print:        eval time =    3443.65 ms /   121 runs   (   28.46 ms per token,    35.14 tokens per second)
llama_perf_context_print:       total time =    6237.21 ms /  3111 tokens
Llama.generate: 3884 prefix-match hit, remaining 174 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     438.19 ms /   450 tokens (    0.97 ms per token,  1026.95 tokens per second)
llama_perf_context_print:        eval time =    4733.20 ms /   170 runs   (   27.84 ms per token,    35.92 tokens per second)
llama_perf_context_print:       total time =    5399.14 ms /   620 tokens
Llama.generate: 3886 prefix-match hit, remaining 199 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     614.50 ms /   642 tokens (    0.96 ms per token,  1044.76 tokens per second)
llama_perf_context_print:        eval time =    4475.76 ms /   159 runs   (   28.15 ms per token,    35.52 tokens per second)
llama_perf_context_print:       total time =    5306.85 ms /   801 tokens
Llama.generate: 3883 prefix-match hit, remaining 1935 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     191.07 ms /   174 tokens (    1.10 ms per token,   910.67 tokens per second)
llama_perf_context_print:        eval time =    1559.36 ms /    56 runs   (   27.85 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    1824.25 ms /   230 tokens
Llama.generate: 3884 prefix-match hit, remaining 327 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     217.05 ms /   199 tokens (    1.09 ms per token,   916.86 tokens per second)
llama_perf_context_print:        eval time =    2060.22 ms /    74 runs   (   27.84 ms per token,    35.92 tokens per second)
llama_perf_context_print:       total time =    2374.41 ms /   273 tokens
Llama.generate: 3883 prefix-match hit, remaining 576 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     318.68 ms /   327 tokens (    0.97 ms per token,  1026.09 tokens per second)
llama_perf_context_print:        eval time =    2642.66 ms /    95 runs   (   27.82 ms per token,    35.95 tokens per second)
llama_perf_context_print:       total time =    3084.53 ms /   422 tokens
Llama.generate: 3884 prefix-match hit, remaining 185 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     537.60 ms /   576 tokens (    0.93 ms per token,  1071.43 tokens per second)
llama_perf_context_print:        eval time =    2480.62 ms /    89 runs   (   27.87 ms per token,    35.88 tokens per second)
llama_perf_context_print:       total time =    3135.25 ms /   665 tokens
Llama.generate: 3883 prefix-match hit, remaining 2403 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     194.34 ms /   185 tokens (    1.05 ms per token,   951.92 tokens per second)
llama_perf_context_print:        eval time =    1950.26 ms /    70 runs   (   27.86 ms per token,    35.89 tokens per second)
llama_perf_context_print:       total time =    2235.41 ms /   255 tokens
Llama.generate: 3886 prefix-match hit, remaining 101 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1732.05 ms /  1935 tokens (    0.90 ms per token,  1117.17 tokens per second)
llama_perf_context_print:        eval time =    4229.78 ms /   149 runs   (   28.39 ms per token,    35.23 tokens per second)
llama_perf_context_print:       total time =    6164.91 ms /  2084 tokens
Llama.generate: 3883 prefix-match hit, remaining 539 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     130.92 ms /   101 tokens (    1.30 ms per token,   771.46 tokens per second)
llama_perf_context_print:        eval time =    1500.08 ms /    54 runs   (   27.78 ms per token,    36.00 tokens per second)
llama_perf_context_print:       total time =    1701.93 ms /   155 tokens
Llama.generate: 3884 prefix-match hit, remaining 187 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     193.21 ms /   187 tokens (    1.03 ms per token,   967.84 tokens per second)
llama_perf_context_print:        eval time =    1726.98 ms /    62 runs   (   27.85 ms per token,    35.90 tokens per second)
llama_perf_context_print:       total time =    2001.36 ms /   249 tokens
Llama.generate: 3884 prefix-match hit, remaining 611 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     514.64 ms /   539 tokens (    0.95 ms per token,  1047.33 tokens per second)
llama_perf_context_print:        eval time =    3256.98 ms /   116 runs   (   28.08 ms per token,    35.62 tokens per second)
llama_perf_context_print:       total time =    3929.98 ms /   655 tokens
Llama.generate: 3883 prefix-match hit, remaining 780 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     575.30 ms /   611 tokens (    0.94 ms per token,  1062.06 tokens per second)
llama_perf_context_print:        eval time =    2868.67 ms /   103 runs   (   27.85 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    3577.53 ms /   714 tokens
Llama.generate: 3884 prefix-match hit, remaining 703 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    2127.99 ms /  2403 tokens (    0.89 ms per token,  1129.23 tokens per second)
llama_perf_context_print:        eval time =    6675.37 ms /   236 runs   (   28.29 ms per token,    35.35 tokens per second)
llama_perf_context_print:       total time =    9129.49 ms /  2639 tokens
Llama.generate: 3884 prefix-match hit, remaining 1749 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     725.24 ms /   780 tokens (    0.93 ms per token,  1075.50 tokens per second)
llama_perf_context_print:        eval time =    3984.19 ms /   142 runs   (   28.06 ms per token,    35.64 tokens per second)
llama_perf_context_print:       total time =    4902.77 ms /   922 tokens
Llama.generate: 3883 prefix-match hit, remaining 1028 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     637.72 ms /   703 tokens (    0.91 ms per token,  1102.37 tokens per second)
llama_perf_context_print:        eval time =    3044.27 ms /   109 runs   (   27.93 ms per token,    35.80 tokens per second)
llama_perf_context_print:       total time =    3823.78 ms /   812 tokens
Llama.generate: 3884 prefix-match hit, remaining 1763 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     938.21 ms /  1028 tokens (    0.91 ms per token,  1095.70 tokens per second)
llama_perf_context_print:        eval time =    4345.82 ms /   155 runs   (   28.04 ms per token,    35.67 tokens per second)
llama_perf_context_print:       total time =    5493.65 ms /  1183 tokens
Llama.generate: 3884 prefix-match hit, remaining 3146 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1547.78 ms /  1749 tokens (    0.88 ms per token,  1130.01 tokens per second)
llama_perf_context_print:        eval time =    6662.25 ms /   237 runs   (   28.11 ms per token,    35.57 tokens per second)
llama_perf_context_print:       total time =    8537.60 ms /  1986 tokens
Llama.generate: 3883 prefix-match hit, remaining 1622 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1569.56 ms /  1763 tokens (    0.89 ms per token,  1123.24 tokens per second)
llama_perf_context_print:        eval time =    6840.15 ms /   243 runs   (   28.15 ms per token,    35.53 tokens per second)
llama_perf_context_print:       total time =    8741.72 ms /  2006 tokens
Llama.generate: 3884 prefix-match hit, remaining 472 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1445.20 ms /  1622 tokens (    0.89 ms per token,  1122.34 tokens per second)
llama_perf_context_print:        eval time =    3567.11 ms /   127 runs   (   28.09 ms per token,    35.60 tokens per second)
llama_perf_context_print:       total time =    5179.34 ms /  1749 tokens
Llama.generate: 3886 prefix-match hit, remaining 718 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    2785.30 ms /  3146 tokens (    0.89 ms per token,  1129.50 tokens per second)
llama_perf_context_print:        eval time =    5479.16 ms /   192 runs   (   28.54 ms per token,    35.04 tokens per second)
llama_perf_context_print:       total time =    8527.49 ms /  3338 tokens
Llama.generate: 3884 prefix-match hit, remaining 1334 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     443.72 ms /   472 tokens (    0.94 ms per token,  1063.74 tokens per second)
llama_perf_context_print:        eval time =    4427.30 ms /   159 runs   (   27.84 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    5080.01 ms /   631 tokens
Llama.generate: 3884 prefix-match hit, remaining 360 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     661.57 ms /   718 tokens (    0.92 ms per token,  1085.30 tokens per second)
llama_perf_context_print:        eval time =    3351.44 ms /   120 runs   (   27.93 ms per token,    35.81 tokens per second)
llama_perf_context_print:       total time =    4170.34 ms /   838 tokens
Llama.generate: 3883 prefix-match hit, remaining 473 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1201.92 ms /  1334 tokens (    0.90 ms per token,  1109.89 tokens per second)
llama_perf_context_print:        eval time =    3510.78 ms /   125 runs   (   28.09 ms per token,    35.60 tokens per second)
llama_perf_context_print:       total time =    4878.76 ms /  1459 tokens
Llama.generate: 3884 prefix-match hit, remaining 1144 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     348.82 ms /   360 tokens (    0.97 ms per token,  1032.04 tokens per second)
llama_perf_context_print:        eval time =    3172.58 ms /   114 runs   (   27.83 ms per token,    35.93 tokens per second)
llama_perf_context_print:       total time =    3669.08 ms /   474 tokens
Llama.generate: 3884 prefix-match hit, remaining 798 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     445.23 ms /   473 tokens (    0.94 ms per token,  1062.37 tokens per second)
llama_perf_context_print:        eval time =    2172.17 ms /    78 runs   (   27.85 ms per token,    35.91 tokens per second)
llama_perf_context_print:       total time =    2719.58 ms /   551 tokens
Llama.generate: 3884 prefix-match hit, remaining 193 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     216.48 ms /   193 tokens (    1.12 ms per token,   891.52 tokens per second)
llama_perf_context_print:        eval time =    2615.94 ms /    94 runs   (   27.83 ms per token,    35.93 tokens per second)
llama_perf_context_print:       total time =    2955.23 ms /   287 tokens
Llama.generate: 3887 prefix-match hit, remaining 2038 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1033.70 ms /  1144 tokens (    0.90 ms per token,  1106.71 tokens per second)
llama_perf_context_print:        eval time =    3225.43 ms /   115 runs   (   28.05 ms per token,    35.65 tokens per second)
llama_perf_context_print:       total time =    4411.23 ms /  1259 tokens
Llama.generate: 3884 prefix-match hit, remaining 3153 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     723.05 ms /   798 tokens (    0.91 ms per token,  1103.66 tokens per second)
llama_perf_context_print:        eval time =    7063.04 ms /   253 runs   (   27.92 ms per token,    35.82 tokens per second)
llama_perf_context_print:       total time =    8133.20 ms /  1051 tokens
Llama.generate: 3884 prefix-match hit, remaining 146 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     172.33 ms /   146 tokens (    1.18 ms per token,   847.19 tokens per second)
llama_perf_context_print:        eval time =    1806.39 ms /    65 runs   (   27.79 ms per token,    35.98 tokens per second)
llama_perf_context_print:       total time =    2063.76 ms /   211 tokens
Llama.generate: 3884 prefix-match hit, remaining 2550 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1777.73 ms /  2038 tokens (    0.87 ms per token,  1146.41 tokens per second)
llama_perf_context_print:        eval time =    5243.38 ms /   186 runs   (   28.19 ms per token,    35.47 tokens per second)
llama_perf_context_print:       total time =    7271.49 ms /  2224 tokens
Llama.generate: 3887 prefix-match hit, remaining 135 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     169.48 ms /   135 tokens (    1.26 ms per token,   796.55 tokens per second)
llama_perf_context_print:        eval time =    3112.50 ms /   112 runs   (   27.79 ms per token,    35.98 tokens per second)
llama_perf_context_print:       total time =    3428.33 ms /   247 tokens
Llama.generate: 3884 prefix-match hit, remaining 482 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    2794.47 ms /  3153 tokens (    0.89 ms per token,  1128.30 tokens per second)
llama_perf_context_print:        eval time =    7277.72 ms /   255 runs   (   28.54 ms per token,    35.04 tokens per second)
llama_perf_context_print:       total time =   10430.36 ms /  3408 tokens
Llama.generate: 3884 prefix-match hit, remaining 214 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     445.84 ms /   482 tokens (    0.92 ms per token,  1081.11 tokens per second)
llama_perf_context_print:        eval time =    2061.13 ms /    74 runs   (   27.85 ms per token,    35.90 tokens per second)
llama_perf_context_print:       total time =    2603.81 ms /   556 tokens
Llama.generate: 3883 prefix-match hit, remaining 3215 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     219.84 ms /   214 tokens (    1.03 ms per token,   973.45 tokens per second)
llama_perf_context_print:        eval time =    3732.33 ms /   134 runs   (   27.85 ms per token,    35.90 tokens per second)
llama_perf_context_print:       total time =    4130.75 ms /   348 tokens
Llama.generate: 3884 prefix-match hit, remaining 1588 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    2225.45 ms /  2550 tokens (    0.87 ms per token,  1145.84 tokens per second)
llama_perf_context_print:        eval time =    7593.76 ms /   268 runs   (   28.33 ms per token,    35.29 tokens per second)
llama_perf_context_print:       total time =   10189.25 ms /  2818 tokens
Llama.generate: 3884 prefix-match hit, remaining 703 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1422.25 ms /  1588 tokens (    0.90 ms per token,  1116.54 tokens per second)
llama_perf_context_print:        eval time =    4160.00 ms /   148 runs   (   28.11 ms per token,    35.58 tokens per second)
llama_perf_context_print:       total time =    5779.54 ms /  1736 tokens
Llama.generate: 3884 prefix-match hit, remaining 964 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     637.91 ms /   703 tokens (    0.91 ms per token,  1102.03 tokens per second)
llama_perf_context_print:        eval time =    4548.68 ms /   163 runs   (   27.91 ms per token,    35.83 tokens per second)
llama_perf_context_print:       total time =    5401.69 ms /   866 tokens
Llama.generate: 3884 prefix-match hit, remaining 2259 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    2850.21 ms /  3215 tokens (    0.89 ms per token,  1127.99 tokens per second)
llama_perf_context_print:        eval time =    5894.91 ms /   207 runs   (   28.48 ms per token,    35.12 tokens per second)
llama_perf_context_print:       total time =    9026.78 ms /  3422 tokens
Llama.generate: 3883 prefix-match hit, remaining 2093 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     882.70 ms /   964 tokens (    0.92 ms per token,  1092.11 tokens per second)
llama_perf_context_print:        eval time =    5209.11 ms /   186 runs   (   28.01 ms per token,    35.71 tokens per second)
llama_perf_context_print:       total time =    6343.29 ms /  1150 tokens
Llama.generate: 3884 prefix-match hit, remaining 3470 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1996.31 ms /  2259 tokens (    0.88 ms per token,  1131.59 tokens per second)
llama_perf_context_print:        eval time =    3282.86 ms /   116 runs   (   28.30 ms per token,    35.34 tokens per second)
llama_perf_context_print:       total time =    5430.37 ms /  2375 tokens
Llama.generate: 3884 prefix-match hit, remaining 1695 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1854.78 ms /  2093 tokens (    0.89 ms per token,  1128.44 tokens per second)
llama_perf_context_print:        eval time =    5924.45 ms /   210 runs   (   28.21 ms per token,    35.45 tokens per second)
llama_perf_context_print:       total time =    8065.44 ms /  2303 tokens
Llama.generate: 3883 prefix-match hit, remaining 1306 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1502.46 ms /  1695 tokens (    0.89 ms per token,  1128.15 tokens per second)
llama_perf_context_print:        eval time =    4361.92 ms /   155 runs   (   28.14 ms per token,    35.53 tokens per second)
llama_perf_context_print:       total time =    6069.63 ms /  1850 tokens
Llama.generate: 3884 prefix-match hit, remaining 983 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1165.15 ms /  1306 tokens (    0.89 ms per token,  1120.88 tokens per second)
llama_perf_context_print:        eval time =    3388.47 ms /   121 runs   (   28.00 ms per token,    35.71 tokens per second)
llama_perf_context_print:       total time =    4711.98 ms /  1427 tokens
Llama.generate: 3883 prefix-match hit, remaining 2020 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    3087.45 ms /  3470 tokens (    0.89 ms per token,  1123.91 tokens per second)
llama_perf_context_print:        eval time =    7807.69 ms /   273 runs   (   28.60 ms per token,    34.97 tokens per second)
llama_perf_context_print:       total time =   11280.77 ms /  3743 tokens
Llama.generate: 3884 prefix-match hit, remaining 1074 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     884.36 ms /   983 tokens (    0.90 ms per token,  1111.54 tokens per second)
llama_perf_context_print:        eval time =    4501.04 ms /   161 runs   (   27.96 ms per token,    35.77 tokens per second)
llama_perf_context_print:       total time =    5597.78 ms /  1144 tokens
Llama.generate: 3884 prefix-match hit, remaining 1574 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1775.20 ms /  2020 tokens (    0.88 ms per token,  1137.90 tokens per second)
llama_perf_context_print:        eval time =    6819.69 ms /   242 runs   (   28.18 ms per token,    35.49 tokens per second)
llama_perf_context_print:       total time =    8930.17 ms /  2262 tokens
Llama.generate: 3884 prefix-match hit, remaining 1824 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     977.00 ms /  1074 tokens (    0.91 ms per token,  1099.28 tokens per second)
llama_perf_context_print:        eval time =    4985.72 ms /   178 runs   (   28.01 ms per token,    35.70 tokens per second)
llama_perf_context_print:       total time =    6202.76 ms /  1252 tokens
Llama.generate: 3883 prefix-match hit, remaining 1210 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1405.12 ms /  1574 tokens (    0.89 ms per token,  1120.19 tokens per second)
llama_perf_context_print:        eval time =    4689.11 ms /   167 runs   (   28.08 ms per token,    35.61 tokens per second)
llama_perf_context_print:       total time =    6316.22 ms /  1741 tokens
Llama.generate: 3884 prefix-match hit, remaining 891 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     794.18 ms /   891 tokens (    0.89 ms per token,  1121.92 tokens per second)
llama_perf_context_print:        eval time =    3382.93 ms /   121 runs   (   27.96 ms per token,    35.77 tokens per second)
llama_perf_context_print:       total time =    4334.90 ms /  1012 tokens
Llama.generate: 3884 prefix-match hit, remaining 204 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1079.94 ms /  1210 tokens (    0.89 ms per token,  1120.44 tokens per second)
llama_perf_context_print:        eval time =    4211.23 ms /   150 runs   (   28.07 ms per token,    35.62 tokens per second)
llama_perf_context_print:       total time =    5491.44 ms /  1360 tokens
Llama.generate: 3883 prefix-match hit, remaining 2377 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1609.34 ms /  1824 tokens (    0.88 ms per token,  1133.38 tokens per second)
llama_perf_context_print:        eval time =    4781.30 ms /   170 runs   (   28.13 ms per token,    35.56 tokens per second)
llama_perf_context_print:       total time =    6618.12 ms /  1994 tokens
Llama.generate: 3884 prefix-match hit, remaining 59 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =      90.31 ms /    59 tokens (    1.53 ms per token,   653.33 tokens per second)
llama_perf_context_print:        eval time =     252.93 ms /     9 runs   (   28.10 ms per token,    35.58 tokens per second)
llama_perf_context_print:       total time =     360.65 ms /    68 tokens
Llama.generate: 3884 prefix-match hit, remaining 454 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     218.68 ms /   204 tokens (    1.07 ms per token,   932.89 tokens per second)
llama_perf_context_print:        eval time =    3505.05 ms /   126 runs   (   27.82 ms per token,    35.95 tokens per second)
llama_perf_context_print:       total time =    3887.92 ms /   330 tokens
Llama.generate: 3884 prefix-match hit, remaining 2273 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     440.00 ms /   454 tokens (    0.97 ms per token,  1031.82 tokens per second)
llama_perf_context_print:        eval time =    3037.52 ms /   109 runs   (   27.87 ms per token,    35.88 tokens per second)
llama_perf_context_print:       total time =    3619.35 ms /   563 tokens
Llama.generate: 3884 prefix-match hit, remaining 1286 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    2099.33 ms /  2377 tokens (    0.88 ms per token,  1132.26 tokens per second)
llama_perf_context_print:        eval time =    4248.20 ms /   150 runs   (   28.32 ms per token,    35.31 tokens per second)
llama_perf_context_print:       total time =    6547.77 ms /  2527 tokens
Llama.generate: 3884 prefix-match hit, remaining 1279 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    1155.52 ms /  1286 tokens (    0.90 ms per token,  1112.92 tokens per second)
llama_perf_context_print:        eval time =    4562.44 ms /   163 runs   (   27.99 ms per token,    35.73 tokens per second)
llama_perf_context_print:       total time =    5934.35 ms /  1449 tokens
Llama.generate: 3884 prefix-match hit, remaining 167 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    2017.73 ms /  2273 tokens (    0.89 ms per token,  1126.52 tokens per second)
llama_perf_context_print:        eval time =    4604.32 ms /   163 runs   (   28.25 ms per token,    35.40 tokens per second)
llama_perf_context_print:       total time =    6837.53 ms /  2436 tokens
Llama.generate: 3884 prefix-match hit, remaining 1161 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     190.66 ms /   167 tokens (    1.14 ms per token,   875.93 tokens per second)
llama_perf_context_print:        eval time =     973.48 ms /    35 runs   (   27.81 ms per token,    35.95 tokens per second)
llama_perf_context_print:       total time =    1212.38 ms /   202 tokens
Llama.generate: 3884 prefix-match hit, remaining 2294 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1129.54 ms /  1279 tokens (    0.88 ms per token,  1132.32 tokens per second)
llama_perf_context_print:        eval time =    4151.66 ms /   148 runs   (   28.05 ms per token,    35.65 tokens per second)
llama_perf_context_print:       total time =    5478.73 ms /  1427 tokens
Llama.generate: 3884 prefix-match hit, remaining 1662 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1054.73 ms /  1161 tokens (    0.91 ms per token,  1100.75 tokens per second)
llama_perf_context_print:        eval time =    4678.72 ms /   167 runs   (   28.02 ms per token,    35.69 tokens per second)
llama_perf_context_print:       total time =    5954.44 ms /  1328 tokens
Llama.generate: 3884 prefix-match hit, remaining 743 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1479.42 ms /  1662 tokens (    0.89 ms per token,  1123.41 tokens per second)
llama_perf_context_print:        eval time =    3183.91 ms /   113 runs   (   28.18 ms per token,    35.49 tokens per second)
llama_perf_context_print:       total time =    4812.94 ms /  1775 tokens
Llama.generate: 3883 prefix-match hit, remaining 584 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    2020.69 ms /  2294 tokens (    0.88 ms per token,  1135.26 tokens per second)
llama_perf_context_print:        eval time =    5165.90 ms /   183 runs   (   28.23 ms per token,    35.42 tokens per second)
llama_perf_context_print:       total time =    7431.29 ms /  2477 tokens
Llama.generate: 3883 prefix-match hit, remaining 2949 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =     682.98 ms /   743 tokens (    0.92 ms per token,  1087.88 tokens per second)
llama_perf_context_print:        eval time =    3460.55 ms /   124 runs   (   27.91 ms per token,    35.83 tokens per second)
llama_perf_context_print:       total time =    4305.01 ms /   867 tokens
Llama.generate: 3884 prefix-match hit, remaining 2279 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =     548.42 ms /   584 tokens (    0.94 ms per token,  1064.88 tokens per second)
llama_perf_context_print:        eval time =    3401.11 ms /   122 runs   (   27.88 ms per token,    35.87 tokens per second)
llama_perf_context_print:       total time =    4111.26 ms /   706 tokens
Llama.generate: 3883 prefix-match hit, remaining 1646 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    2017.03 ms /  2279 tokens (    0.89 ms per token,  1129.88 tokens per second)
llama_perf_context_print:        eval time =    4040.78 ms /   143 runs   (   28.26 ms per token,    35.39 tokens per second)
llama_perf_context_print:       total time =    6246.32 ms /  2422 tokens
Llama.generate: 3884 prefix-match hit, remaining 1188 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1463.83 ms /  1646 tokens (    0.89 ms per token,  1124.45 tokens per second)
llama_perf_context_print:        eval time =    4700.40 ms /   167 runs   (   28.15 ms per token,    35.53 tokens per second)
llama_perf_context_print:       total time =    6388.68 ms /  1813 tokens
Llama.generate: 3884 prefix-match hit, remaining 1984 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =    2628.55 ms /  2949 tokens (    0.89 ms per token,  1121.91 tokens per second)
llama_perf_context_print:        eval time =    6051.88 ms /   213 runs   (   28.41 ms per token,    35.20 tokens per second)
llama_perf_context_print:       total time =    8967.80 ms /  3162 tokens
Llama.generate: 3883 prefix-match hit, remaining 871 prompt tokens to eval
llama_perf_context_print:        load time =    3931.91 ms
llama_perf_context_print: prompt eval time =    1737.58 ms /  1984 tokens (    0.88 ms per token,  1141.82 tokens per second)
llama_perf_context_print:        eval time =    3562.46 ms /   126 runs   (   28.27 ms per token,    35.37 tokens per second)
llama_perf_context_print:       total time =    5467.43 ms /  2110 tokens
Llama.generate: 3883 prefix-match hit, remaining 642 prompt tokens to eval
llama_perf_context_print:        load time =    3839.82 ms
llama_perf_context_print: prompt eval time =    1075.07 ms /  1188 tokens (    0.90 ms per token,  1105.05 tokens per second)
llama_perf_context_print:        eval time =    5462.66 ms /   195 runs   (   28.01 ms per token,    35.70 tokens per second)
llama_perf_context_print:       total time =    6798.93 ms /  1383 tokens
Llama.generate: 3884 prefix-match hit, remaining 1123 prompt tokens to eval
llama_perf_context_print:        load time =    3419.34 ms
llama_perf_context_print: prompt eval time =     791.81 ms /   871 tokens (    0.91 ms per token,  1100.01 tokens per second)
llama_perf_context_print:        eval time =    4858.70 ms /   174 runs   (   27.92 ms per token,    35.81 tokens per second)
llama_perf_context_print:       total time =    5881.77 ms /  1045 tokens
Llama.generate: 3883 prefix-match hit, remaining 1435 prompt tokens to eval
slurmstepd: error: *** JOB 11245026 ON gcn108 CANCELLED AT 2025-04-17T15:57:10 DUE TO TIME LIMIT ***
