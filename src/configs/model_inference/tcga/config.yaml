tokenizer:
  tokenizer_name: "meta-llama/Meta-Llama-3-8B"
model:
  model_path: "./pathollama_train/checkpoint-5240/"
  vocab_size: 128256
  hidden_size: 768
  num_hidden_layers: 2
  num_attention_heads: 2
  max_position_embeddings: 4096

dataset:
  pickle_file_path: "./data/tcga_data/tcga_titan_embeddings_reports_processed.pkl"
  embeddings_dim_size: 768
  max_seq_length: 2048

inference:
  model_path: "./pathollama_train/checkpoint-4000-2-layers/model.safetensors"
  temperature: 0.5
  top_k: 64
  min_p: 0.00
  top_p: 0.95
  max_new_tokens: 2048
  do_sample: false


