tokenizer:
  tokenizer_name: "./tokenizers/trained_tokenizers/32768_pubmed/"
  custom_tokenizer: true
model:
  model_path: "meta-llama/Meta-Llama-3-8B"
  vocab_size: 32768
  hidden_size: 768
  num_hidden_layers: 2
  num_attention_heads: 2
  max_position_embeddings: 1024
  cross_attn: true
  cross_attn_layer_index: 1

dataset:
  val_pickle_file_path: "./data/tcga_data/split/Llama-3.3-70B-processed-200-split-variations/tcga_val_variations.pkl"
  embeddings_dim_size: 768
  max_seq_length: 1024
  random_choice_report: false

inference:
  model_path: "./pathollama_train/checkpoint-2500-cross-attention-2nd-layer/model.safetensors"
  temperature: 0.8
  top_k: 128
  min_p: 0.00
  top_p: 0.95
  max_new_tokens: 1024
  do_sample: false


