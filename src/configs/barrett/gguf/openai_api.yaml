# Add this line to select the local Llama-cpp engine
engine: "openai" 
model_id: "Llama 3.3 70b Instruct AWQ"

max_new_tokens: 32768
api_base: "https://willma.liza.surf.nl/api/v0/" 
api_key: "AI_HUB_KEY"

# The rest of your parameters
max_seq_length: 128000
max_new_tokens: 4096
seed: None
temperature: 0.6
top_k: 256
min_p: 0.00
top_p: 0.95
n_gpu_layers: 81
chat_format: "llama-3"
repetition_penalty: 1.0