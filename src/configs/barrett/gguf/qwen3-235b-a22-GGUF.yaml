# Add this line to select the local Llama-cpp engine
engine: "llama_cpp" 

# The rest of your parameters
model_path: "../../models/hf_models/qwen_235b_GGUF/Q4_K_M/Qwen3-235B-A22B-Q4_K_M-00001-of-00003.gguf"
type: "qwen_reasoning"
max_seq_length: 128000
max_new_tokens: 4096
seed: 42
temperature: 0.6
top_k: 20
min_p: 0.00
top_p: 0.8
n_gpu_layers: 95
chat_format: "qwen"
repetition_penalty: 1.0