# Report Generation and Evaluation Scripts

This section of the repository contains scripts for generating clinical reports using a multimodal language model (`PathoLlamaForCausalLM`) and evaluating their semantic similarity to ground truth reports using the CONCH model.

## Scripts Overview

### 1. `generate_val_reports.py`
This script runs inference with a trained multimodal language model on a validation dataset to generate clinical reports.

#### Functionality:
- Loads configuration from a YAML file.
- Loads the model and tokenizer from a specified checkpoint.
- Reads the validation dataset containing WSI embeddings and report variations.
- For each patient:
  - Selects a report variation as reference.
  - Generates a report using the model.
  - Saves the original and generated reports.
- Outputs all results in a JSON file.

#### Running the script:
```sh
python model_eval/generate_val_reports.py \
  --config ./configs/model_inference/tcga/config.yaml \
  --output ./data/tcga_data/tcga_generated/tcga_200_val_6_layers.json
```

#### Output:
- A JSON file where each entry contains:
  - `patient_id`
  - `original_report`: A real clinical report from the dataset.
  - `generated_report`: The report generated by the model.

---

### 2. `conch_evaluation.py`
This script evaluates the similarity between the original and generated reports using the CONCH model.

#### Functionality:
- Loads the CONCH model from a checkpoint.
- Computes text embeddings for the original and generated reports.
- Calculates cosine similarity between the embeddings.
- Outputs statistics (mean and standard deviation) of the similarity scores.

#### Running the script:
```sh
python model_eval/conch_evaluation.py \
  --checkpoint_path ./model_eval/CONCH/checkpoints/conch/pytorch_model.bin \
  --input_json ./data/tcga_data/tcga_generated/tcga_200_val_6_layers.json
```

#### Output:
- Mean cosine similarity and standard deviation across all evaluated pairs.

#### Example Output:
```
Processed 200 report pairs.
Mean cosine similarity: 0.8235
Standard deviation:     0.0451
```

---

These scripts provide an end-to-end evaluation pipeline: from generating reports using a multimodal model to quantitatively assessing their semantic similarity to ground truth reports.

