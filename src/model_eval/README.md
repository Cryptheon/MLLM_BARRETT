# Report Generation and Evaluation Scripts

This section of the repository contains scripts for generating clinical reports using a multimodal language model (`PathoLlamaForCausalLM`) and evaluating their semantic similarity to ground truth reports using the CONCH model.

## Scripts Overview

### 1. `generate_val_reports.py`
This script runs inference with a trained multimodal language model on a validation dataset to generate clinical reports.

#### Functionality:
- Loads configuration from a YAML file.
- Loads the model and tokenizer from a specified checkpoint.
- Reads the validation dataset containing WSI embeddings and report variations.
- For each patient:
  - Selects a report variation as reference.
  - Generates a report using the model.
  - Saves the original and generated reports.
- Outputs all results in a JSON file.

#### Running the script:
```sh
python model_eval/generate_val_reports.py \
  --config ./configs/model_inference/tcga/config.yaml \
  --output ./data/tcga_data/tcga_generated/tcga_200_val_6_layers.json
```

#### Output:
- A JSON file where each entry contains:
  - `patient_id`
  - `original_report`: A real clinical report from the dataset.
  - `generated_report`: The report generated by the model.

---

### 2. `conch_evaluation.py`
This script evaluates the similarity between the original and generated reports using the CONCH model.

#### Functionality:
- Loads the CONCH model from a checkpoint.
- Computes text embeddings for the original and generated reports.
- Calculates cosine similarity between the embeddings.
- Outputs statistics (mean and standard deviation) of the similarity scores.

#### Running the script:
```sh
python model_eval/conch_evaluation.py \
  --checkpoint_path ./model_eval/CONCH/checkpoints/conch/pytorch_model.bin \
  --input_json ./data/tcga_data/tcga_generated/tcga_200_val_6_layers.json
```

#### Output:
- Mean cosine similarity and standard deviation across all evaluated pairs.

#### Example Output:
```
Processed 200 report pairs.
Mean cosine similarity: 0.8235
Standard deviation:     0.0451
```

---

### 3. `evaluate_tcga_classification.py`
This script evaluates the cancer type classification accuracy by comparing model-predicted labels against ground truth labels using TCGA data.

#### Functionality:
- Loads predicted labels and ground truth mappings.
- Computes classification report and accuracy.
- Optionally computes multiclass AUC and confusion matrix.

#### Running the script:
```sh
python model_eval/evaluate_tcga_classification.py \
  --predictions_json ./data/tcga_data/tcga_generated/extracted_labels/tcga_200_val.json \
  --ground_truth_csv ./data/tcga_data/tcga_labels/tcga_patient_to_cancer_type.csv \
  --tcga_json ./configs/tcga/tcga_labels.json
```

#### Output:
- Classification metrics including accuracy, precision, recall, F1-score.
- Optionally: AUC (macro) and ROC curve plots.

---

### 4. `evaluate_model_fidelity.py`
This script evaluates structured fidelity scores from raw LLM output, parsing embedded JSON with detailed scoring per component.

#### Functionality:
- Parses raw model evaluation strings to extract JSON-formatted rubric scores.
- Computes descriptive statistics on fidelity scores and their components.
- Outputs a summary CSV and a boxplot of per-category scores.

#### Running the script:
```sh
python evaluate_model_fidelity.py \
  --fidelity_json ../data/tcga_data/tcga_generated/evaluated_rubric/llama_70b_eval.json
```

#### Output:
- Console statistics on overall and per-category fidelity scores.
- A CSV summary of per-case scores.
- A boxplot saved as `category_scores_boxplot.png`.

---

These scripts provide an end-to-end evaluation pipeline: from generating reports using a multimodal model to quantitatively assessing their semantic similarity, classification accuracy, and structured fidelity against rubric-based criteria.

