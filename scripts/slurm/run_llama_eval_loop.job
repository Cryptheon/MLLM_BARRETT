#!/bin/bash

#SBATCH --job-name=llama_eval_loop    # Job name
#SBATCH --partition=gpu_h100          # Partition
#SBATCH --time=06:00:00               # Max runtime
#SBATCH --gpus=2                      # Request 2 GPUs
#SBATCH --output=slurm_logs/llama_reports_%j.out
#SBATCH --error=slurm_logs/llama_reports_%j.err

# --- Job Setup ---
echo "=========================================================="
echo "Job started on $(date)"
echo "Job ID: $SLURM_JOB_ID"
echo "Running on nodes: $SLURM_JOB_NODELIST"
echo "=========================================================="

# Create log dir if it doesn't exist (relative to where sbatch is run, usually root)
mkdir -p slurm_logs

# --- Load Required Modules ---
echo "Loading modules..."
module purge
module load 2024
module load CMake/3.29.3-GCCcore-13.3.0
module load OpenSSL/3
module load cuDNN/9.5.0.50-CUDA-12.6.0

# --- Environment Setup ---
# 1. Navigate to the source directory
# Note: Adjust this path if your specific cluster path differs
PROJECT_SRC="/projects/0/prjs1597/LMM/MLLM_BARRETT/src"
cd "$PROJECT_SRC" || { echo "Failed to cd to $PROJECT_SRC"; exit 1; }

# 2. Activate the Python virtual environment
# Assumes venv is at MLLM_BARRETT/venv relative to src
echo "Activating virtual environment..."
source ../venv/bin/activate

# --- Configuration ---
# Modify these variables to change the evaluation range
START_POINT=400
END_POINT=1500
INCREMENT=50

# Paths relative to 'src/' (since we cd'd there)
BASE_MODEL_PATH="./trained/mid_lr_pretrained_qwen_tcga_structured_revision"
CONFIG="./configs/barrett/eval_config.yaml"
GGUF_MODEL="./configs/barrett/gguf/Llama-3.3-70B-instruct-GGUF.yaml"
GT_LABELS="./configs/labels/reference_full_real_report_labels_revision_24_10_25.json"

# Output base relative to src
OUTPUT_BASE_DIR="../../data/results/${BASE_MODEL_PATH}"

echo "Starting evaluation loop: ${START_POINT} -> ${END_POINT} (Step: ${INCREMENT})"

# --- Evaluation Loop ---
for i in $(seq ${START_POINT} ${INCREMENT} ${END_POINT})
do
  # Define the path to the specific checkpoint model file
  MODEL_PATH="${BASE_MODEL_PATH}/checkpoint-${i}/model.safetensors"

  # Check if the model file actually exists before trying to run
  if [ -f "$MODEL_PATH" ]; then
    echo "----------------------------------------------------"
    echo "Processing Checkpoint-${i}"
    echo "----------------------------------------------------"
    
    python -m model_eval.run_eval_pipeline \
      --config "$CONFIG" \
      --model "$MODEL_PATH" \
      --gguf_model "$GGUF_MODEL" \
      --output_base_dir "$OUTPUT_BASE_DIR" \
      --source_report_labels "$GT_LABELS"
      
    echo "Finished evaluation for checkpoint-${i}"
  else
    echo "----------------------------------------------------"
    echo "Skipping checkpoint-${i}: Model file not found at ${MODEL_PATH}"
    echo "----------------------------------------------------"
  fi
done

echo "ðŸŽ‰ All evaluations are complete."
