# Scripts Directory Utility Guide

This directory contains utility scripts for data preprocessing, feature inspection, PubMed data retrieval, and HPC (Slurm) job configurations for the MLLM Barrett project.

## Python Scripts

### 1. `combine_h5_files.py`
* **What it does:** Merges multiple HDF5 (`.h5`) files into a single consolidated file. This is typically used to combine patch features or embeddings extracted in parallel across different nodes.
* **How to run:**
    ```bash
    python scripts/combine_h5_files.py --input_dir /path/to/h5_chunks --output_file consolidated_features.h5
    ```
* **What to expect:** A single `.h5` file containing the combined datasets (e.g., `features`, `coords`) from all source files in the input directory.

### 2. `convert_xml_to_json.py`
* **What it does:** Parses pathology reports or metadata stored in XML format (common in TCGA datasets) and converts them into a structured JSON format.
* **How to run:**
    ```bash
    python scripts/convert_xml_to_json.py --xml_path report.xml --output_path report.json
    ```
* **What to expect:** A structured JSON file mapped from the XML schema, making it easier to load into Python dictionaries for LLM processing.

### 3. `download_pubmed_abstracts_filtered.py`
* **What it does:** Uses the Entrez API to download abstracts from PubMed. It filters results based on keywords defined in `src/data/keywords/pubmed/histopathology_pubmed_keywords.txt` to ensure relevance to histopathology.
* **How to run:**
    ```bash
    python scripts/download_pubmed_abstracts_filtered.py --output_dir ./data/pubmed_raw --email your@email.com
    ```
* **What to expect:** A collection of text or JSON files containing thousands of medical abstracts relevant to pathology for pre-training or fine-tuning.

### 4. `extract_images_from_tar.py`
* **What it does:** Efficiently extracts image files (JPG, PNG) from large `.tar` archives without needing to extract the entire archive if only specific files are needed.
* **How to run:**
    ```bash
    python scripts/extract_images_from_tar.py --tar_path slides.tar --out_dir ./extracted_images
    ```
* **What to expect:** The target directory populated with images extracted from the archive.

### 5. `inspect_conch_titan_features.py`
* **What it does:** A debugging tool to inspect the contents of H5 feature files generated by CONCH or Titan extractors. It prints dataset keys, shapes, and sample metadata.
* **How to run:**
    ```bash
    python scripts/inspect_conch_titan_features.py --h5_path path/to/features.h5
    ```
* **What to expect:** Console output detailing the tensor dimensions and attributes of the stored embeddings.

### 6. `inspect_tar.py`
* **What it does:** Quickly lists the contents and file structure of a `.tar` file.
* **How to run:**
    ```bash
    python scripts/inspect_tar.py --tar_path data.tar
    ```
* **What to expect:** A printout of all file paths contained within the archive.

### 7. `inspect_tcga_titan_demo_data.py`
* **What it does:** Verification script specifically for the TCGA Titan demo data. It checks for the existence of required feature vectors and their alignment with clinical labels.
* **How to run:**
    ```bash
    python scripts/inspect_tcga_titan_demo_data.py
    ```
* **What to expect:** A summary report on data integrity and availability for the demo slides.

---

## Slurm Job Scripts (`scripts/slurm/`)

These files are used for submitting tasks to an HPC cluster using the Slurm workload manager.

### `extract_patches_slide_embeddings_trident.job`
* **Purpose:** Runs the patch extraction and embedding pipeline using the Trident extractor.
* **Usage:** `sbatch scripts/slurm/extract_patches_slide_embeddings_trident.job`

### `run_full_eval_loop.job`
* **Purpose:** Automates the evaluation process across multiple checkpoints. It iterates through model weights and runs the evaluation suite.
* **Usage:** `sbatch scripts/slurm/run_full_eval_loop.job`

### `run_pathology_inference.job`
* **Purpose:** Submits a high-memory/GPU task to perform inference on WSIs, generating diagnostic reports using the trained PathoLlama model.
* **Usage:** `sbatch scripts/slurm/run_pathology_inference.job`

---

## Other Scripts
* `tokenizers/scripts/train_tokenizer.py`: Used to train a custom SentencePiece or BPE tokenizer on pathology-specific text (like the PubMed abstracts) to improve model vocabulary for medical terms.